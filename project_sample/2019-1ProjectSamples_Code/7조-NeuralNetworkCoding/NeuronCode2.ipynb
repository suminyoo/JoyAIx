{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.48200334 -5.97502112 -5.78805802  1.53827965]\n",
      " [ 4.84653739 -3.63623586 -7.150584    2.0106567 ]\n",
      " [ 2.63142419  6.91539118  2.85681316  0.66437678]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "\n",
    "alpha,hidden_dim= (0.5,4)\n",
    "synapse_0 = 2*np.random.random((3,hidden_dim))-1\n",
    "synapse_1 = 2*np.random.random((hidden_dim,1))-1\n",
    "\n",
    "for j in range(60000):\n",
    "    layer_1 = 1/(1+np.exp(-(np.dot(X, synapse_0))))\n",
    "    layer_2 = 1/(1+np.exp(-(np.dot(layer_1, synapse_1))))\n",
    "    layer_2_delta = (layer_2-y)*(layer_2*(1-layer_2))\n",
    "    layer_1_delta = layer_2_delta.dot(synapse_1.T)*(layer_1*(1-layer_1))\n",
    "    synapse_1 -= (alpha * layer_1.T.dot(layer_2_delta))\n",
    "    synapse_0 -= (alpha * X.T.dot(layer_1_delta))\n",
    "    \n",
    "print (synapse_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <2레이어 뉴럴네트워크>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트레이닝 후 결과는\n",
      "[[0.00505119]\n",
      " [0.00505119]\n",
      " [0.99494905]\n",
      " [0.99494905]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# 시그모이드 비선형을 계산\n",
    "def sigmoid(x): \n",
    "    output = 1/(1+np.exp(-x)) \n",
    "    return output \n",
    "\n",
    "# 시그모이드 함수 결과값을 미분값으로 전환\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "# 인풋 데이터 셋\n",
    "X = np.array([ [0,1], \n",
    "                [0,1],\n",
    "                [1,0],\n",
    "                [1,0] ])\n",
    "\n",
    "# 결과 데이터 셋\n",
    "y = np.array([[0,0,1,1]]).T \n",
    "\n",
    "# 실험의 편의를 위해 항상 같은 값이 나오게 함\n",
    "np.random.seed(1) \n",
    "\n",
    "# 웨이트들을 평균 0인 수들로 무작위로 초기화\n",
    "synapse_0 = 2*np.random.random((2,1))-1 \n",
    "\n",
    "for iter in range(10000): \n",
    "\n",
    "    # 먼저 전파하기 \n",
    "    layer_0 = X \n",
    "    layer_1 = sigmoid(np.dot(layer_0, synapse_0))\n",
    "                      \n",
    "    # 얼마나 놓쳤을까? \n",
    "    layer_1_error = layer_1 - y\n",
    "\n",
    "    # l1 안에서 값에서의 시그모이드 경사와 놓친 값들을 곱해주기 \n",
    "    layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1) \n",
    "    synapse_0_derivative = np.dot(layer_0.T, layer_1_delta)\n",
    "\n",
    "    # 웨이트 업데이트\n",
    "    synapse_0 -= synapse_0_derivative\n",
    "\n",
    "print ('트레이닝 후 결과는')\n",
    "print (layer_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <경사하강법 향상시키기>\n",
    "#### * 현재 “x” 위치의 “경사”를 계산\n",
    "#### * 맨 밑 두 줄 x 값을 경사의 -값으로 바꿈 (x=x-slope)\n",
    "#### * 경사가 0이 될 때까지 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "알파값을 이용한 트레이닝 0.001\n",
      "에러 후 0반복 횟수0.49641003190272537\n",
      "에러 후 10000반복 횟수0.49516402549338606\n",
      "에러 후 20000반복 횟수0.4935960431880486\n",
      "에러 후 30000반복 횟수0.4916063585594306\n",
      "에러 후 40000반복 횟수0.48910016654420474\n",
      "에러 후 50000반복 횟수0.48597785784615843\n",
      "\n",
      "알파값을 이용한 트레이닝 0.01\n",
      "에러 후 0반복 횟수0.49641003190272537\n",
      "에러 후 10000반복 횟수0.45743107444190134\n",
      "에러 후 20000반복 횟수0.359097202563399\n",
      "에러 후 30000반복 횟수0.23935813715897253\n",
      "에러 후 40000반복 횟수0.1430706590133703\n",
      "에러 후 50000반복 횟수0.09859642980892719\n",
      "\n",
      "알파값을 이용한 트레이닝 0.1\n",
      "에러 후 0반복 횟수0.49641003190272537\n",
      "에러 후 10000반복 횟수0.042888017000115755\n",
      "에러 후 20000반복 횟수0.02409899422852161\n",
      "에러 후 30000반복 횟수0.018110652146797843\n",
      "에러 후 40000반복 횟수0.014987616272210912\n",
      "에러 후 50000반복 횟수0.013014490538142586\n",
      "\n",
      "알파값을 이용한 트레이닝 1\n",
      "에러 후 0반복 횟수0.49641003190272537\n",
      "에러 후 10000반복 횟수0.008584525653247159\n",
      "에러 후 20000반복 횟수0.0057894598625078085\n",
      "에러 후 30000반복 횟수0.004629176776769985\n",
      "에러 후 40000반복 횟수0.003958765280273649\n",
      "에러 후 50000반복 횟수0.0035101225678616766\n",
      "\n",
      "알파값을 이용한 트레이닝 10\n",
      "에러 후 0반복 횟수0.49641003190272537\n",
      "에러 후 10000반복 횟수0.003129388763011837\n",
      "에러 후 20000반복 횟수0.002144595579852179\n",
      "에러 후 30000반복 횟수0.0017239754995622308\n",
      "에러 후 40000반복 횟수0.0014782145122908034\n",
      "에러 후 50000반복 횟수0.0013127406283356764\n",
      "\n",
      "알파값을 이용한 트레이닝 100\n",
      "에러 후 0반복 횟수0.49641003190272537\n",
      "에러 후 10000반복 횟수0.12547698383358538\n",
      "에러 후 20000반복 횟수0.12533033354043677\n",
      "에러 후 30000반복 횟수0.12526772879373652\n",
      "에러 후 40000반복 횟수0.12523107370012884\n",
      "에러 후 50000반복 횟수0.12520635280373757\n",
      "\n",
      "알파값을 이용한 트레이닝 1000\n",
      "에러 후 0반복 횟수0.49641003190272537\n",
      "에러 후 10000반복 횟수0.5\n",
      "에러 후 20000반복 횟수0.5\n",
      "에러 후 30000반복 횟수0.5\n",
      "에러 후 40000반복 횟수0.5\n",
      "에러 후 50000반복 횟수0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# 시그모이드 비선형성을 사용해서 계산\n",
    "\n",
    "def sigmoid(x): \n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# 시그모이드 함수의 결과값을 이용해 미분값으로 변환\n",
    "\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "X = np.array([[0,0,1],\n",
    "             [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1]\n",
    "             ])\n",
    "\n",
    "y = np.array([[0],\n",
    "             [1],\n",
    "             [1],\n",
    "             [0]])\n",
    "\n",
    "for alpha in alphas:\n",
    "    print (\"\\n알파값을 이용한 트레이닝 \"+ str(alpha))\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # 랜덤적으로 웨이트값들을 평균 0으로 초기화\n",
    "    synapse_0 = 2*np.random.random((3,4)) - 1 \n",
    "    synapse_1 = 2*np.random.random((4,1)) - 1\n",
    "    \n",
    "    for j in range(60000):\n",
    "        # 레이어 0, 1, 2로 값을 부여\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0, synapse_0)) \n",
    "        layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
    "        \n",
    "        # 얼마나 놓쳤을까?\n",
    "        layer_2_error = layer_2 - y\n",
    "        \n",
    "        if (j%10000) == 0:\n",
    "            print (\"에러 후 \" + str(j) + \"반복 횟수\" + str(np.mean(np.abs(layer_2_error))))\n",
    "        \n",
    "        # 타겟 내의 방향은 무엇인가요?\n",
    "        # 정말 확신할 수 있나요? 그렇다면 많이 바꾸면 안된다. \n",
    "        \n",
    "        layer_2_delta = layer_2_error*sigmoid_output_to_derivative(layer_2)\n",
    "        \n",
    "        \n",
    "        # (웨이트에 따르면) 얼마나 각각의 l1 값들은 l2에러에 기여했을까? \n",
    "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "        \n",
    "        # l1 타겟 내의 방향은 무엇인가요?\n",
    "        # 정말 확신할 수 있나요?\n",
    "        \n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "        \n",
    "        ##### 여기가 중요!!! 바뀐 부분\n",
    "        synapse_1 -= alpha * (layer_1.T.dot(layer_2_delta))\n",
    "        synapse_0 -= alpha * (layer_0.T.dot(layer_1_delta))\n",
    "        \n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "알파값을 이용한 트레이닝 0.001\n",
      "에러: 0.49641003190272537\n",
      "에러: 0.49516402549338606\n",
      "에러: 0.4935960431880486\n",
      "에러: 0.4916063585594306\n",
      "에러: 0.48910016654420474\n",
      "에러: 0.48597785784615843\n",
      "synaspe 0은 \n",
      " [[-0.28448441  0.32471214 -1.53496167 -0.47594822]\n",
      " [-0.7550616  -1.04593014 -1.45446052 -0.32606771]\n",
      " [-0.2594825  -0.13487028 -0.29722666  0.40028038]]\n",
      "synapse 0 방향변화를 업데이트하면, \n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 1. 1.]]\n",
      "Synapse 1은 \n",
      "  [[-0.61957526]\n",
      " [ 0.76414675]\n",
      " [-1.49797046]\n",
      " [ 0.40734574]]\n",
      "synapse 1 방향변화를 업데이트하면, \n",
      "  [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "\n",
      "알파값을 이용한 트레이닝 0.01\n",
      "에러: 0.49641003190272537\n",
      "에러: 0.45743107444190134\n",
      "에러: 0.359097202563399\n",
      "에러: 0.23935813715897253\n",
      "에러: 0.1430706590133703\n",
      "에러: 0.09859642980892719\n",
      "synaspe 0은 \n",
      " [[ 2.39225985  2.56885428 -5.38289334 -3.29231397]\n",
      " [-0.35379718 -4.6509363  -5.67005693 -1.74287864]\n",
      " [-0.15431323 -1.17147894  1.97979367  3.44633281]]\n",
      "synapse 0 방향변화를 업데이트하면, \n",
      " [[1. 1. 0. 0.]\n",
      " [2. 0. 0. 2.]\n",
      " [4. 2. 1. 1.]]\n",
      "Synapse 1은 \n",
      "  [[-3.70045078]\n",
      " [ 4.57578637]\n",
      " [-7.63362462]\n",
      " [ 4.73787613]]\n",
      "synapse 1 방향변화를 업데이트하면, \n",
      "  [[2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "\n",
      "알파값을 이용한 트레이닝 0.1\n",
      "에러: 0.49641003190272537\n",
      "에러: 0.042888017000115755\n",
      "에러: 0.02409899422852161\n",
      "에러: 0.018110652146797843\n",
      "에러: 0.014987616272210912\n",
      "에러: 0.013014490538142586\n",
      "synaspe 0은 \n",
      " [[ 3.88035459  3.6391263  -5.99509098 -3.8224267 ]\n",
      " [-1.72462557 -5.41496387 -6.30737281 -3.03987763]\n",
      " [ 0.45953952 -1.77301389  2.37235987  5.04309824]]\n",
      "synapse 0 방향변화를 업데이트하면, \n",
      " [[1. 1. 0. 0.]\n",
      " [2. 0. 0. 2.]\n",
      " [4. 2. 1. 1.]]\n",
      "Synapse 1은 \n",
      "  [[-5.72386389]\n",
      " [ 6.15041318]\n",
      " [-9.40272079]\n",
      " [ 6.61461026]]\n",
      "synapse 1 방향변화를 업데이트하면, \n",
      "  [[2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "\n",
      "알파값을 이용한 트레이닝 1\n",
      "에러: 0.49641003190272537\n",
      "에러: 0.008584525653247159\n",
      "에러: 0.0057894598625078085\n",
      "에러: 0.004629176776769985\n",
      "에러: 0.003958765280273649\n",
      "에러: 0.0035101225678616766\n",
      "synaspe 0은 \n",
      " [[ 4.6013571   4.17197193 -6.30956245 -4.19745118]\n",
      " [-2.58413484 -5.81447929 -6.60793435 -3.68396123]\n",
      " [ 0.97538679 -2.02685775  2.52949751  5.84371739]]\n",
      "synapse 0 방향변화를 업데이트하면, \n",
      " [[1. 1. 0. 0.]\n",
      " [2. 0. 0. 2.]\n",
      " [4. 2. 1. 1.]]\n",
      "Synapse 1은 \n",
      "  [[ -6.96765763]\n",
      " [  7.14101949]\n",
      " [-10.31917382]\n",
      " [  7.86128405]]\n",
      "synapse 1 방향변화를 업데이트하면, \n",
      "  [[2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "\n",
      "알파값을 이용한 트레이닝 10\n",
      "에러: 0.49641003190272537\n",
      "에러: 0.003129388763011837\n",
      "에러: 0.002144595579852179\n",
      "에러: 0.0017239754995622308\n",
      "에러: 0.0014782145122908034\n",
      "에러: 0.0013127406283356764\n",
      "synaspe 0은 \n",
      " [[ 4.52597806  5.77663165 -7.34266481 -5.29379829]\n",
      " [ 1.66715206 -7.16447274 -7.99779235 -1.81881849]\n",
      " [-4.27032921 -3.35838279  3.44594007  4.88852208]]\n",
      "synapse 0 방향변화를 업데이트하면, \n",
      " [[ 7. 19.  2.  6.]\n",
      " [ 7.  2.  0. 22.]\n",
      " [19. 26.  9. 17.]]\n",
      "Synapse 1은 \n",
      "  [[ -8.58485788]\n",
      " [ 10.1786297 ]\n",
      " [-14.87601886]\n",
      " [  7.57026121]]\n",
      "synapse 1 방향변화를 업데이트하면, \n",
      "  [[22.]\n",
      " [15.]\n",
      " [ 4.]\n",
      " [15.]]\n",
      "\n",
      "알파값을 이용한 트레이닝 100\n",
      "에러: 0.49641003190272537\n",
      "에러: 0.12547698383358538\n",
      "에러: 0.12533033354043677\n",
      "에러: 0.12526772879373652\n",
      "에러: 0.12523107370012884\n",
      "에러: 0.12520635280373757\n",
      "synaspe 0은 \n",
      " [[-17.20515383   1.89881344 -16.95533169  -8.23482697]\n",
      " [  5.7023927  -17.23785048  -9.48052434  -7.92972576]\n",
      " [ -4.18780303  -0.33881828   2.8202323   -8.40059859]]\n",
      "synapse 0 방향변화를 업데이트하면, \n",
      " [[ 8.  7.  3.  2.]\n",
      " [13.  8.  2.  4.]\n",
      " [16. 13. 12.  8.]]\n",
      "Synapse 1은 \n",
      "  [[  9.68285305]\n",
      " [  9.5573212 ]\n",
      " [-16.03906837]\n",
      " [  6.27326973]]\n",
      "synapse 1 방향변화를 업데이트하면, \n",
      "  [[13.]\n",
      " [11.]\n",
      " [12.]\n",
      " [10.]]\n",
      "\n",
      "알파값을 이용한 트레이닝 1000\n",
      "에러: 0.49641003190272537\n",
      "에러: 0.5\n",
      "에러: 0.5\n",
      "에러: 0.5\n",
      "에러: 0.5\n",
      "에러: 0.5\n",
      "synaspe 0은 \n",
      " [[-56.06177241  -4.66916407  -5.65196178 -23.05868769]\n",
      " [ -4.52271708  -4.78629811 -10.887702   -15.85879101]\n",
      " [-89.56678495  10.61497652  37.02351519 -48.33299795]]\n",
      "synapse 0 방향변화를 업데이트하면, \n",
      " [[3. 2. 4. 1.]\n",
      " [1. 2. 2. 1.]\n",
      " [6. 6. 4. 1.]]\n",
      "Synapse 1은 \n",
      "  [[  25.16188889]\n",
      " [  -8.65053733]\n",
      " [-104.60697286]\n",
      " [  11.41582458]]\n",
      "synapse 1 방향변화를 업데이트하면, \n",
      "  [[7.]\n",
      " [7.]\n",
      " [7.]\n",
      " [3.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# 시그모이드 비선형성을 사용해서 계산\n",
    "def sigmoid(x): \n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# 시그모이드 함수의 결과값을 이용해 미분값으로 변환 \n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "X = np.array([[0,0,1],\n",
    "             [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1]\n",
    "             ])\n",
    "\n",
    "y = np.array([[0],\n",
    "             [1],\n",
    "             [1],\n",
    "             [0]])\n",
    "\n",
    "for alpha in alphas:\n",
    "    print (\"\\n알파값을 이용한 트레이닝 \"+ str(alpha))\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # 랜덤적으로 웨이트값들을 평균 0으로 초기화\n",
    "    synapse_0 = 2*np.random.random((3,4)) - 1 \n",
    "    synapse_1 = 2*np.random.random((4,1)) - 1\n",
    "    \n",
    "    prev_synapse_0_weight_update = np.zeros_like(synapse_0)\n",
    "    prev_synapse_1_weight_update = np.zeros_like(synapse_1)\n",
    "    \n",
    "    synapse_0_direction_count = np.zeros_like(synapse_0)\n",
    "    synapse_1_direction_count = np.zeros_like(synapse_1)\n",
    "    \n",
    "    for j in range(60000):\n",
    "        # 레이어 0, 1, 2로 값을 부여\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0, synapse_0)) \n",
    "        layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
    "        \n",
    "        # 얼마나 target 값들을 놓쳤을까?\n",
    "        layer_2_error = y - layer_2\n",
    "        \n",
    "        if (j%10000) == 0:\n",
    "            print ( \"에러: \" + str(np.mean(np.abs(layer_2_error))) ) \n",
    "        \n",
    "    \n",
    "        # 타겟 내의 방향은 무엇일까?\n",
    "        # 우리가 정말 맞을까? 만약, 그렇다면 많이 바꾸지 않아야한다. \n",
    "        layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2)\n",
    "    \n",
    "        # (웨이트에 의하면) 각각의 l1 값들은 얼마나 l2에러에 영향을 미쳤을까?\n",
    "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "        \n",
    "        # 타겟 l1 내의 방향은 무엇일까?\n",
    "        # 우리가 정말 맞을까? 만약, 그렇다면 많이 바꾸지 않아야한다. \n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "        \n",
    "        synapse_1_weight_update = (layer_1.T.dot(layer_2_delta))\n",
    "        synapse_0_weight_update = (layer_0.T.dot(layer_1_delta))\n",
    "    \n",
    "\n",
    "        if (j > 0):\n",
    "            synapse_0_direction_count += np.abs(((synapse_0_weight_update > 0)+0) - ((prev_synapse_0_weight_update > 0) + 0))\n",
    "            synapse_1_direction_count += np.abs(((synapse_1_weight_update > 0)+0) - ((prev_synapse_1_weight_update > 0) + 0))  \n",
    "            \n",
    "        synapse_1 += alpha * synapse_1_weight_update\n",
    "        synapse_0 += alpha * synapse_0_weight_update\n",
    "        \n",
    "        prev_synapse_0_weight_update = synapse_0_weight_update\n",
    "        prev_synapse_1_weight_update = synapse_1_weight_update\n",
    "        \n",
    "    print (\"synaspe 0은 \\n\", synapse_0)\n",
    "    \n",
    "    print (\"synapse 0 방향변화를 업데이트하면, \\n\", synapse_0_direction_count)\n",
    "    \n",
    "    print (\"Synapse 1은 \\n \", synapse_1)\n",
    "    \n",
    "    print (\"synapse 1 방향변화를 업데이트하면, \\n \", synapse_1_direction_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <히든 레이어의 크기들을 파라미터화 하기>\n",
    "\n",
    "#### → 히든레이어의 크기를 키우는 것이 가능하다면, 각각의 반복 또한 수렴하는 검색범위를 증가시킬 수 있음 \n",
    "\n",
    "#### * 알파값이 매우 작으면, 미분하더라도 방향은 거의 바뀌지 않음\n",
    "#### * 알파값이 최적값이라면, 미분하면서 방향이 여러번 바뀜\n",
    "#### * 알파값이 매우 커지면, 미분하면서 방향이 중간값으로 바뀜\n",
    "#### * 알파값이 매우 작으면, 웨이트 또한 상당히 작아지면서 마무리됨\n",
    "#### * 알파값이 매우 커지면, 웨이트 또한 매우 커짐\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "알파값을 이용한 트레이닝 0.001\n",
      "에러 이후 0 반복 후: 0.49643992250078794\n",
      "에러 이후 10000 반복 후: 0.49104946812904954\n",
      "에러 이후 20000 반복 후: 0.4849763070274596\n",
      "에러 이후 30000 반복 후: 0.4778306787926556\n",
      "에러 이후 40000 반복 후: 0.4690384653902826\n",
      "에러 이후 50000 반복 후: 0.458029258565275\n",
      "\n",
      "알파값을 이용한 트레이닝 0.01\n",
      "에러 이후 0 반복 후: 0.49643992250078794\n",
      "에러 이후 10000 반복 후: 0.35637906164802124\n",
      "에러 이후 20000 반복 후: 0.14693984546476\n",
      "에러 이후 30000 반복 후: 0.08801561274158767\n",
      "에러 이후 40000 반복 후: 0.06514781927504912\n",
      "에러 이후 50000 반복 후: 0.052965808702569714\n",
      "\n",
      "알파값을 이용한 트레이닝 0.1\n",
      "에러 이후 0 반복 후: 0.49643992250078794\n",
      "에러 이후 10000 반복 후: 0.030540490838555055\n",
      "에러 이후 20000 반복 후: 0.01906387253341843\n",
      "에러 이후 30000 반복 후: 0.014764390729581685\n",
      "에러 이후 40000 반복 후: 0.012389242990471297\n",
      "에러 이후 50000 반복 후: 0.010842166973777436\n",
      "\n",
      "알파값을 이용한 트레이닝 1\n",
      "에러 이후 0 반복 후: 0.49643992250078794\n",
      "에러 이후 10000 반복 후: 0.00736052234249372\n",
      "에러 이후 20000 반복 후: 0.004972517050388168\n",
      "에러 이후 30000 반복 후: 0.003968639781590644\n",
      "에러 이후 40000 반복 후: 0.003386410219831655\n",
      "에러 이후 50000 반복 후: 0.002996256849322485\n",
      "\n",
      "알파값을 이용한 트레이닝 10\n",
      "에러 이후 0 반복 후: 0.49643992250078794\n",
      "에러 이후 10000 반복 후: 0.002254538839644365\n",
      "에러 이후 20000 반복 후: 0.0015372348629016657\n",
      "에러 이후 30000 반복 후: 0.00123439543710358\n",
      "에러 이후 40000 반복 후: 0.0010580718173873289\n",
      "에러 이후 50000 반복 후: 0.0009395364918713383\n",
      "\n",
      "알파값을 이용한 트레이닝 100\n",
      "에러 이후 0 반복 후: 0.49643992250078794\n",
      "에러 이후 10000 반복 후: 0.5\n",
      "에러 이후 20000 반복 후: 0.5\n",
      "에러 이후 30000 반복 후: 0.5\n",
      "에러 이후 40000 반복 후: 0.5\n",
      "에러 이후 50000 반복 후: 0.5\n",
      "\n",
      "알파값을 이용한 트레이닝 1000\n",
      "에러 이후 0 반복 후: 0.49643992250078794\n",
      "에러 이후 10000 반복 후: 0.5\n",
      "에러 이후 20000 반복 후: 0.5\n",
      "에러 이후 30000 반복 후: 0.5\n",
      "에러 이후 40000 반복 후: 0.5\n",
      "에러 이후 50000 반복 후: 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "hiddenSize = 32\n",
    "\n",
    "# 시그모이드 비선형성을 사용해서 계산\n",
    "def sigmoid(x): \n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# 시그모이드 함수의 결과값을 이용해 미분값으로 변환\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "X = np.array([[0,0,1],\n",
    "             [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1]\n",
    "             ])\n",
    "\n",
    "y = np.array([[0],\n",
    "             [1],\n",
    "             [1],\n",
    "             [0]])\n",
    "\n",
    "for alpha in alphas:\n",
    "    print (\"\\n알파값을 이용한 트레이닝 \"+ str(alpha))\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # 랜덤적으로 웨이트값들을 평균 0으로 초기화\n",
    "    synapse_0 = 2*np.random.random((3, hiddenSize)) - 1\n",
    "    synapse_1 = 2*np.random.random((hiddenSize, 1)) - 1\n",
    "    \n",
    "    for j in range(60000):\n",
    "        # 레이어 0, 1, 2로 값을 부여\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0, synapse_0)) \n",
    "        layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
    "        \n",
    "        # 얼마나 target 값들을 놓쳤을까?\n",
    "        layer_2_error = layer_2 - y\n",
    "        if (j % 10000) == 0:\n",
    "            print(\"에러 이후 \" + str(j) + \" 반복 후: \" + str(np.mean(np.abs(layer_2_error))))\n",
    "\n",
    "        \n",
    "        # 타겟 내의 방향은 무엇일까?\n",
    "        # 우리가 정말 맞을까? 만약, 그렇다면 많이 바꾸지 않아야한다. \n",
    "        layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2)\n",
    "        \n",
    "        # (웨이트에 의하면) 각각의 l1 값들은 얼마나 l2에러에 영향을 미쳤을까?\n",
    "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "        \n",
    "        # 타겟 l1 내의 방향은 무엇일까?\n",
    "        # 우리가 정말 맞을까? 만약, 그렇다면 많이 바꾸지 않아야한다. \n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "        \n",
    "        synapse_1 -= alpha * (layer_1.T.dot(layer_2_delta))\n",
    "        synapse_0 -= alpha * (layer_0.T.dot(layer_1_delta))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
