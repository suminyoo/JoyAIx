{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1학년 수강 과목을 통한 학부 예측 프로그램 만들기  \n",
    "## 모두를 위한 인공지능 Final project - 11조 윤희원(21600481)/김유빈(21900167)  \n",
    "### 프로젝트 소개  \n",
    "수업시간에 배운 MNIST BGD 모델을 활용하여 한동대학교 18학번 학생들의 데이터를 학습시킨 뒤 실제 선택 학부를 얼마나 잘 예측하는 지 확인한다.  \n",
    "  \n",
    "### 프로젝트 목표  \n",
    "1. 18학번들의 수강과목 데이터를 활용하여 2학년이 되었을때 어떤 학부를 선택할지 예측할 수 있다.  \n",
    "2. MNIST 모델을 사용한 학습 모델의 구조를 이해할 수 있다.  \n",
    "3. 18학번이 수강한 과목 데이터를 통해 어떤 학부 전공을 선택하게 될지 직접 인공지능 모델을 학습시킬 수 있다.  \n",
    "  \n",
    "### 프로젝트 개요\n",
    "1. '모두를 위한 인공지능의 활용’ 수업 github에서 JoyML11-3BatchGD.ipynb 파일을 다운로드 한다.  \n",
    "2. MNIST BGD 모델 코드를 활용하여 우리가 실제 학습하고자하는 신경망의 형태로 변환한다.  \n",
    "3. 18학번들의 수강과목 데이터를 다운받아서 필요한 정보들만 추출하여 CSV 형태로 가공한다.  \n",
    "4. 가공된 데이터를 Jupyter Notebook으로 읽어들인 후 학습시킨다.  \n",
    "5. 테스트용 데이터를 활용하여 예측 정확도를 확인한다.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST BGD 모델의 활용  \n",
    "바로 아래의 셀은 MNIST BGD 모델에서 활용하는 함수들을 정의 해놓은 영역이다. 초기 가중치들의 설정, 순전파, 역전파 등의 알고리즘이 잘 정리되어 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistBGD_LS(object):\n",
    "    \"\"\" Batch Gradient Descent with Learning Schedule\n",
    "    \"\"\"\n",
    "    def __init__(self, n_x, n_h1, n_y, eta = 0.1, epochs = 100, random_seed=1):\n",
    "        \"\"\" \n",
    "        \"\"\"\n",
    "        self.n_x = n_x\n",
    "        self.n_h = n_h\n",
    "        self.n_y = n_y\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        np.random.seed(random_seed)\n",
    "        self.W1 = 2*np.random.random((self.n_h, self.n_x)) - 1  # between -1 and 1\n",
    "        self.W2 = 2*np.random.random((self.n_y, self.n_h)) - 1  # between -1 and 1\n",
    "        print('W1.shape={}, W2.shape={}'.format(self.W1.shape, self.W2.shape))\n",
    "        \n",
    "    def forpass(self, A0):\n",
    "        Z1 = np.dot(self.W1, A0)        # hidden layer inputs\n",
    "        A1 = self.g(Z1)                 # hidden layer outputs/activation func\n",
    "        Z2 = np.dot(self.W2, A1)        # output layer inputs\n",
    "        A2 = self.g(Z2)                 # output layer outputs/activation func\n",
    "        return Z1, A1, Z2, A2\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.cost_ = []\n",
    "        self.m_samples = len(y)       \n",
    "        Y = joy.one_hot_encoding(y, self.n_y)     \n",
    "        # learning rate is scheduled to decrement by a step of \n",
    "        # which the inteveral from self.eta to 0.0001 eqaully \n",
    "        # divided by total number of iterations(epochs or \n",
    "        # epochs * m_samples)\n",
    "        eta_scheduled = np.linspace(self.eta, 0.0001, self.epochs)\n",
    "        \n",
    "        # for momentum\n",
    "        #self.v1 = np.zeros_like(self.W1)\n",
    "        #self.v2 = np.zeros_like(self.W2)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            if epoch % 1000 == 0:\n",
    "                print('Training epoch {}/{}.'.format(epoch, self.epochs))\n",
    "\n",
    "            A0 = np.array(X, ndmin=2).T       \n",
    "            Y0 = np.array(Y, ndmin=2).T     \n",
    "\n",
    "            Z1, A1, Z2, A2 = self.forpass(A0)  \n",
    "            E2 = Y0 - A2                      \n",
    "            E1 = np.dot(self.W2.T, E2)         \n",
    "\n",
    "            dZ2 = E2 * self.g_prime(Z2)          \n",
    "            dZ1 = E1 * self.g_prime(Z1)       \n",
    "            \n",
    "            # udpate weight with momentum\n",
    "            #eta = learning_schedule[epoch]\n",
    "            #self.v2 = 0.9 * self.v2 + self.eta * np.dot(dZ2, A1.T) / m_samples\n",
    "            #self.v1 = 0.9 * self.v1 + self.eta * np.dot(dZ1, A0.T) / m_samples\n",
    "            #self.W2 += self.v2     \n",
    "            #self.W1 += self.v1 \n",
    "\n",
    "            # update weights without momentum\n",
    "            # eta = eta_scheduled[epoch]\n",
    "            self.W2 +=  self.eta * np.dot(dZ2, A1.T) / self.m_samples    \n",
    "            self.W1 +=  self.eta * np.dot(dZ1, A0.T) / self.m_samples    \n",
    "            self.cost_.append(np.sqrt(np.sum(E2 * E2)))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        A0 = np.array(X, ndmin=2).T         # A0: inputs\n",
    "        Z1, A1, Z2, A2 = self.forpass(A0)   # forpass\n",
    "        return A2                                       \n",
    "\n",
    "    def g(self, x):                 # activation_function: sigmoid\n",
    "        x = np.clip(x, -500, 500)   # prevent from overflow, \n",
    "        return 1.0/(1.0+np.exp(-x)) # stackoverflow.com/questions/23128401/\n",
    "                                    # overflow-error-in-neural-networks-implementation\n",
    "    \n",
    "    def g_prime(self, x):                    # activation_function: sigmoid derivative\n",
    "        return self.g(x) * (1 - self.g(x))\n",
    "    \n",
    "    def evaluate(self, Xtest, ytest):       # fully vectorized calculation\n",
    "        m_samples = len(ytest)\n",
    "        scores = 0        \n",
    "        A2 = self.predict(Xtest)\n",
    "        yhat = np.argmax(A2, axis = 0)\n",
    "        scores += np.sum(yhat == ytest)\n",
    "        return scores/m_samples * 100\n",
    "    \n",
    "    def evaluate_onebyone(self, Xtest, ytest):\n",
    "        m_samples = len(ytest)\n",
    "        scores = 0\n",
    "        for m in range(m_samples):\n",
    "            A2 = nn.predict(Xtest[m])\n",
    "            yhat = np.argmax(A2)\n",
    "            if yhat == ytest[m]:\n",
    "                scores += 1        \n",
    "        return scores/m_samples * 100\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가공된 데이터를 적용하여 학습시키기  \n",
    "아래 셀에서는 CSV 파일의 형태로 가공된 학생들의 수강 과목과 선택 학부 데이터를 활용해 직접 학습을 시킨다.  \n",
    "MNIST classification accuracy를 통해 자체적으로 학습된 데이터의 정확성에 대하여 알아볼 수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape=(100, 200), W2.shape=(12, 100)\n",
      "Training epoch 0/6000.\n",
      "Training epoch 1000/6000.\n",
      "Training epoch 2000/6000.\n",
      "Training epoch 3000/6000.\n",
      "Training epoch 4000/6000.\n",
      "Training epoch 5000/6000.\n",
      "MNIST classification accuracy 98.66666666666667%\n"
     ]
    }
   ],
   "source": [
    "import imp\n",
    "import joy\n",
    "imp.reload(joy)\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data1 = np.genfromtxt(\"C:\\python\\class1.csv\", dtype=int, delimiter=',', names=True, encoding='UTF8')\n",
    "#각 학생이 어떠한 과목을 수강하였는지를 숫자로 표현한 가공 데이터(class1)를 읽어들인다.\n",
    "data2 = np.genfromtxt(\"C:\\python\\major1.csv\", dtype=int, delimiter=',', names=True, encoding='UTF8')\n",
    "#각 학생이 어떠한 학부를 선택하였는지를 숫자로 표현한 가공 데이터(major1)을 읽어들인다.\n",
    "\n",
    "X = np.zeros((600, 200), dtype = np.int32)\n",
    "Y = np.zeros((600), dtype = np.int32)\n",
    "\n",
    "for i in range(600):\n",
    "    for j in range(22): #0으로 초기화된 전체 과목들 중 학생이 수강한 과목의 경우 해당 인덱스의 값을 1로 변환\n",
    "            if  data1[i][j] != 0:\n",
    "                X[i][data1[i][j]] = 1\n",
    "                \n",
    "for k in range(600): #각각의 학생이 최종적으로 선택한 학부의 데이터를 입력\n",
    "    Y[k] = data2[k][0]-1\n",
    "\n",
    "# set hyperparameters and instantiate the class object\n",
    "n_x, n_h, n_y = 200, 100, 12          \n",
    "nn = MnistBGD_LS(n_x, n_h, n_y, eta = 0.2, epochs = 6000)  \n",
    "\n",
    "# train the model\n",
    "nn.fit(X, Y)       \n",
    "accuracy = nn.evaluate(X, Y)      \n",
    "print('MNIST classification accuracy {}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새로운 데이터를 입력 받았을 경우  \n",
    "새로운 학생들의 데이터를 입력 받을 때에 과연 어느 정도의 예측 정확성을 보여주기 위한 부분이다.  \n",
    "이때, 현재 600명의 데이터만을 바탕으로 학습한 신경망으로는 20% 정도의 낮은 정확성만을 보여준다. \n",
    "비록 그렇다 할지라도 단순히 하나의 학부를 선택했을 경우인 1/12의 확률보다는 약 두 배 가량 높은 정확도를 보인다.  \n",
    "따라서 만약 많은 양의 데이터만 확보된다면 이 신경망을 통해 보다 정확한 예측을 수행할 것으로 기대된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST classification accuracy 20.588235294117645%\n"
     ]
    }
   ],
   "source": [
    "data3 = np.genfromtxt(\"C:\\python\\class2.csv\", dtype=int, delimiter=',', names=True, encoding='UTF8')\n",
    "#각 학생이 어떠한 과목을 수강하였는지를 숫자로 표현한 가공 데이터(class2)를 읽어들인다.\n",
    "data4 = np.genfromtxt(\"C:\\python\\major2.csv\", dtype=int, delimiter=',', names=True, encoding='UTF8')\n",
    "#각 학생이 어떠한 학부를 선택하였는지를 숫자로 표현한 가공 데이터(major2)을 읽어들인다.\n",
    "\n",
    "Xtest = np.zeros((34, 200), dtype = np.int32)\n",
    "Ytest = np.zeros((34), dtype = np.int32)\n",
    "\n",
    "for i in range(34):\n",
    "    for j in range(22):\n",
    "            if  data3[i][j] != 0:\n",
    "                Xtest[i][data1[i][j]] = 1\n",
    "                \n",
    "for k in range(34): \n",
    "    Ytest[k] = data4[k][0]-1\n",
    "\n",
    "accuracy = nn.evaluate(Xtest, Ytest) \n",
    "print('MNIST classification accuracy {}%'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
