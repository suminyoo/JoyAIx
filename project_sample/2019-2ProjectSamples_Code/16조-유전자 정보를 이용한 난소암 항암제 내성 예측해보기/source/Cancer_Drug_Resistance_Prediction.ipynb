{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example DNN code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#import lib.dataProcess as dp\n",
    "from pandas import DataFrame as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correct(predict, y): #get a prediction value and real value\n",
    "    result = {}\n",
    "    result['True-Positive'] = 0\n",
    "    result['False-Negative'] = 0\n",
    "    result['True-Negative'] = 0\n",
    "    result['False-Positive'] = 0\n",
    "\n",
    "    for i in range(len(predict)) :\n",
    "        if int(y[i])==1 : # when the patient has resistant \n",
    "            if int(predict[i]) == 1 : # when the prediction is right\n",
    "                result['True-Positive'] += 1\n",
    "            else :                    # when the prediction is wrong\n",
    "                result['False-Negative'] += 1\n",
    "        else :           #when the patient has sensitive\n",
    "            if int(predict[i]) == 0 : # when the prediction is right\n",
    "                result['True-Negative'] += 1\n",
    "            else :                    # when the prediction is wrong\n",
    "                result['False-Positive'] += 1\n",
    "\n",
    "    # calculate sensitivity\n",
    "    sensitivity=result['True-Positive']/(result['True-Positive']+result['False-Negative'])\n",
    "    \n",
    "    # calculate specificity\n",
    "    specificity=result['True-Negative']/(result['True-Negative']+result['False-Positive'])\n",
    "\n",
    "    return sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred): #customized loss function \n",
    "\n",
    "    # calculate true positive\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    # calculate true negative\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    # calculate false positive\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    # calculate false negative\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "    sen=tp/(tp+fn+K.epsilon())\n",
    "    spe=tn/(tn+fp+K.epsilon())\n",
    "                                #K.epsilon()(very small positive number) is prevent from dividing by 0\n",
    "    # calculate balanced accuracy\n",
    "    bal=(sen+spe)/(2+K.epsilon())\n",
    "    # make nan and zero like number as 0\n",
    "    bal = tf.where(tf.is_nan(bal), tf.zeros_like(bal), bal)\n",
    "\n",
    "    print(tf.size(tp),tf.size(tn))\n",
    "    return 1 - K.mean(bal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./TCGA_OV_platinum_status.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f83bb9695d7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./TCGA_OV_platinum_status.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# save patient data in 'data_patient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_patient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./TCGA_OV_platinum_status.csv' does not exist"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./TCGA_OV_platinum_status.csv\")\n",
    "\n",
    "# save patient data in 'data_patient'\n",
    "data_patient = data.patient.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient</th>\n",
       "      <th>ENSG00000211950</th>\n",
       "      <th>ENSG00000259042</th>\n",
       "      <th>ENSG00000178372</th>\n",
       "      <th>ENSG00000120937</th>\n",
       "      <th>ENSG00000150750</th>\n",
       "      <th>ENSG00000156076</th>\n",
       "      <th>ENSG00000211947</th>\n",
       "      <th>ENSG00000091583</th>\n",
       "      <th>ENSG00000254951</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000185966</th>\n",
       "      <th>ENSG00000211659</th>\n",
       "      <th>ENSG00000187492</th>\n",
       "      <th>ENSG00000166415</th>\n",
       "      <th>ENSG00000244437</th>\n",
       "      <th>ENSG00000064218</th>\n",
       "      <th>ENSG00000243775</th>\n",
       "      <th>ENSG00000188817</th>\n",
       "      <th>ENSG00000211818</th>\n",
       "      <th>Platinum_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA-04-1331</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>-1.952</td>\n",
       "      <td>0.438</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>-0.755</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-1.952</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.952</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-1.097</td>\n",
       "      <td>0.413</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.798</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA-04-1332</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>-1.083</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.959</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-1.269</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.359</td>\n",
       "      <td>-1.959</td>\n",
       "      <td>-1.959</td>\n",
       "      <td>-0.557</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA-04-1347</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-1.768</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>-1.768</td>\n",
       "      <td>-1.124</td>\n",
       "      <td>0.924</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>-1.768</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.768</td>\n",
       "      <td>0.667</td>\n",
       "      <td>-0.622</td>\n",
       "      <td>0.531</td>\n",
       "      <td>1.172</td>\n",
       "      <td>-1.768</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>-1.768</td>\n",
       "      <td>-1.768</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA-04-1348</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-1.876</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-1.876</td>\n",
       "      <td>-1.037</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>0.627</td>\n",
       "      <td>-1.037</td>\n",
       "      <td>-1.876</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.876</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.744</td>\n",
       "      <td>0.616</td>\n",
       "      <td>1.533</td>\n",
       "      <td>-0.853</td>\n",
       "      <td>-0.853</td>\n",
       "      <td>-0.744</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA-04-1362</td>\n",
       "      <td>-1.935</td>\n",
       "      <td>-1.935</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-1.935</td>\n",
       "      <td>-0.668</td>\n",
       "      <td>-0.892</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-1.935</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>-0.781</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.718</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-1.262</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TCGA-04-1364</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-1.861</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-1.861</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-1.861</td>\n",
       "      <td>-1.861</td>\n",
       "      <td>-1.031</td>\n",
       "      <td>-1.208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-1.861</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.516</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-1.861</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>0.398</td>\n",
       "      <td>-1.861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TCGA-04-1365</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>-0.607</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>-1.039</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>0.569</td>\n",
       "      <td>-1.039</td>\n",
       "      <td>-1.877</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.877</td>\n",
       "      <td>1.597</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.783</td>\n",
       "      <td>1.266</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-1.217</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TCGA-04-1514</td>\n",
       "      <td>-2.024</td>\n",
       "      <td>-2.024</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.905</td>\n",
       "      <td>-1.158</td>\n",
       "      <td>-2.024</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.024</td>\n",
       "      <td>-2.024</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.438</td>\n",
       "      <td>-0.711</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-1.341</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-2.024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TCGA-04-1517</td>\n",
       "      <td>-0.805</td>\n",
       "      <td>-1.910</td>\n",
       "      <td>-1.910</td>\n",
       "      <td>-0.754</td>\n",
       "      <td>-1.236</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-1.054</td>\n",
       "      <td>-1.910</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.910</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.481</td>\n",
       "      <td>-1.236</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>-1.910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TCGA-04-1530</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-1.997</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>-1.997</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>0.791</td>\n",
       "      <td>-1.997</td>\n",
       "      <td>-1.997</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>1.103</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.854</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-1.997</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TCGA-04-1536</td>\n",
       "      <td>0.733</td>\n",
       "      <td>-1.681</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-1.681</td>\n",
       "      <td>-1.681</td>\n",
       "      <td>-1.681</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-1.681</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-1.681</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-1.050</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-1.681</td>\n",
       "      <td>-1.681</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TCGA-04-1542</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-0.767</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-0.767</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-0.868</td>\n",
       "      <td>-0.868</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TCGA-04-1648</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>-0.557</td>\n",
       "      <td>-0.792</td>\n",
       "      <td>0.245</td>\n",
       "      <td>-0.792</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-1.270</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.941</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.449</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-1.090</td>\n",
       "      <td>0.602</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TCGA-04-1651</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.415</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>1.012</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-1.185</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TCGA-04-1655</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.461</td>\n",
       "      <td>-0.602</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>-0.748</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TCGA-09-0364</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>-0.765</td>\n",
       "      <td>-0.723</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>0.893</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TCGA-09-0366</td>\n",
       "      <td>-1.948</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-1.096</td>\n",
       "      <td>-1.948</td>\n",
       "      <td>-1.948</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-1.096</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.948</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-1.948</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TCGA-09-1662</td>\n",
       "      <td>-0.639</td>\n",
       "      <td>-1.835</td>\n",
       "      <td>0.512</td>\n",
       "      <td>-1.835</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>-0.824</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-1.006</td>\n",
       "      <td>-1.182</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.835</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-1.835</td>\n",
       "      <td>-0.824</td>\n",
       "      <td>0.527</td>\n",
       "      <td>-0.716</td>\n",
       "      <td>-0.675</td>\n",
       "      <td>-1.835</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TCGA-09-1665</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-1.823</td>\n",
       "      <td>-1.823</td>\n",
       "      <td>-0.592</td>\n",
       "      <td>0.659</td>\n",
       "      <td>-1.823</td>\n",
       "      <td>-1.168</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.592</td>\n",
       "      <td>0.666</td>\n",
       "      <td>-1.823</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-1.823</td>\n",
       "      <td>-1.823</td>\n",
       "      <td>-1.823</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TCGA-09-1666</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>-0.603</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>-1.811</td>\n",
       "      <td>-1.811</td>\n",
       "      <td>-0.669</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-1.811</td>\n",
       "      <td>-0.505</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.811</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.816</td>\n",
       "      <td>-1.811</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.467</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TCGA-09-1667</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-1.866</td>\n",
       "      <td>-0.796</td>\n",
       "      <td>-0.747</td>\n",
       "      <td>-1.866</td>\n",
       "      <td>-1.866</td>\n",
       "      <td>1.211</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-1.214</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.866</td>\n",
       "      <td>0.932</td>\n",
       "      <td>-1.214</td>\n",
       "      <td>-1.866</td>\n",
       "      <td>1.355</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>-1.038</td>\n",
       "      <td>-0.856</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TCGA-09-1670</td>\n",
       "      <td>-0.648</td>\n",
       "      <td>-1.899</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-1.899</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>-0.390</td>\n",
       "      <td>0.481</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>-0.648</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.899</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-1.234</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.759</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TCGA-09-2045</td>\n",
       "      <td>-1.801</td>\n",
       "      <td>-1.801</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-1.801</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-1.801</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-1.801</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.801</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.165</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.471</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-1.801</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TCGA-09-2051</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>-0.571</td>\n",
       "      <td>0.472</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>0.502</td>\n",
       "      <td>-1.074</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.712</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>-1.252</td>\n",
       "      <td>-0.551</td>\n",
       "      <td>-0.643</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>TCGA-09-2053</td>\n",
       "      <td>0.212</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-1.974</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>1.328</td>\n",
       "      <td>-1.974</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.974</td>\n",
       "      <td>1.490</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>1.583</td>\n",
       "      <td>-1.110</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>-1.974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TCGA-09-2056</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>0.311</td>\n",
       "      <td>-1.738</td>\n",
       "      <td>-1.099</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>0.820</td>\n",
       "      <td>-0.642</td>\n",
       "      <td>-1.099</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.738</td>\n",
       "      <td>0.359</td>\n",
       "      <td>-1.738</td>\n",
       "      <td>-1.738</td>\n",
       "      <td>0.566</td>\n",
       "      <td>-1.738</td>\n",
       "      <td>-1.738</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>TCGA-10-0926</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>-1.863</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-1.863</td>\n",
       "      <td>-1.015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.829</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.520</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>-1.863</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-1.863</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TCGA-10-0927</td>\n",
       "      <td>0.495</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>0.246</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>0.603</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>0.781</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>TCGA-10-0931</td>\n",
       "      <td>-1.770</td>\n",
       "      <td>-0.665</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.665</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-1.770</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-1.770</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-1.126</td>\n",
       "      <td>-1.126</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.772</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>TCGA-10-0933</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>-0.575</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>0.584</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>0.345</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>TCGA-31-1953</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>-1.920</td>\n",
       "      <td>-1.920</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-1.920</td>\n",
       "      <td>-0.705</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.920</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.525</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-1.257</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>TCGA-36-1568</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>1.041</td>\n",
       "      <td>-1.862</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-1.862</td>\n",
       "      <td>-1.862</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.862</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-1.026</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.584</td>\n",
       "      <td>-1.204</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>TCGA-36-1570</td>\n",
       "      <td>-0.643</td>\n",
       "      <td>-1.886</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>-1.047</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-1.225</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>-1.886</td>\n",
       "      <td>-0.643</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.802</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.863</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-1.886</td>\n",
       "      <td>-1.225</td>\n",
       "      <td>1.008</td>\n",
       "      <td>-1.886</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>TCGA-36-1571</td>\n",
       "      <td>-1.899</td>\n",
       "      <td>-1.899</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>-1.899</td>\n",
       "      <td>-0.453</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>-0.756</td>\n",
       "      <td>-1.899</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.899</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.857</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>0.486</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>TCGA-36-1574</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>0.955</td>\n",
       "      <td>-0.764</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.805</td>\n",
       "      <td>0.762</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.761</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>-0.896</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>TCGA-36-1576</td>\n",
       "      <td>0.705</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>0.398</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>1.135</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>-1.250</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>0.902</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.559</td>\n",
       "      <td>1.859</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>TCGA-36-1578</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>-1.856</td>\n",
       "      <td>-1.856</td>\n",
       "      <td>-1.856</td>\n",
       "      <td>-1.197</td>\n",
       "      <td>1.967</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-0.835</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.856</td>\n",
       "      <td>0.983</td>\n",
       "      <td>-1.856</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>1.533</td>\n",
       "      <td>-0.775</td>\n",
       "      <td>-1.197</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>TCGA-36-1580</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>0.368</td>\n",
       "      <td>-1.949</td>\n",
       "      <td>-1.272</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>0.442</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.949</td>\n",
       "      <td>1.162</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.667</td>\n",
       "      <td>-0.646</td>\n",
       "      <td>-0.839</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>-0.746</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>TCGA-36-1581</td>\n",
       "      <td>1.091</td>\n",
       "      <td>-0.669</td>\n",
       "      <td>1.368</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>-1.002</td>\n",
       "      <td>-1.111</td>\n",
       "      <td>1.060</td>\n",
       "      <td>-1.967</td>\n",
       "      <td>-1.293</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.967</td>\n",
       "      <td>1.321</td>\n",
       "      <td>-1.002</td>\n",
       "      <td>0.790</td>\n",
       "      <td>1.681</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>-0.731</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>TCGA-57-1586</td>\n",
       "      <td>-2.026</td>\n",
       "      <td>-2.026</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>-2.026</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>0.967</td>\n",
       "      <td>-2.026</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.661</td>\n",
       "      <td>1.258</td>\n",
       "      <td>-1.146</td>\n",
       "      <td>-0.617</td>\n",
       "      <td>0.470</td>\n",
       "      <td>-0.756</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>TCGA-61-1725</td>\n",
       "      <td>0.846</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.690</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>-1.783</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>1.221</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-1.783</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.783</td>\n",
       "      <td>0.757</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>1.389</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>TCGA-61-1728</td>\n",
       "      <td>-0.767</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.809</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-1.967</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-1.110</td>\n",
       "      <td>-0.697</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.967</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.599</td>\n",
       "      <td>-1.110</td>\n",
       "      <td>-1.292</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.860</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>TCGA-61-1733</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>-0.731</td>\n",
       "      <td>-1.839</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>-1.839</td>\n",
       "      <td>-1.839</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-1.839</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.839</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>-1.019</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>TCGA-61-1736</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>-0.766</td>\n",
       "      <td>-0.766</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>-0.723</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.668</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.482</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>-0.766</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-1.908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>TCGA-61-1737</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>0.367</td>\n",
       "      <td>-1.865</td>\n",
       "      <td>-1.865</td>\n",
       "      <td>-1.865</td>\n",
       "      <td>0.429</td>\n",
       "      <td>-1.865</td>\n",
       "      <td>-1.865</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.865</td>\n",
       "      <td>0.660</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.705</td>\n",
       "      <td>0.718</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.746</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>TCGA-61-1738</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-1.798</td>\n",
       "      <td>-1.798</td>\n",
       "      <td>-1.798</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>1.250</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-1.798</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.798</td>\n",
       "      <td>1.207</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>1.322</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>-0.699</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>-1.798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>TCGA-61-1741</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-2.048</td>\n",
       "      <td>-0.624</td>\n",
       "      <td>-2.048</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>1.703</td>\n",
       "      <td>-1.060</td>\n",
       "      <td>-1.358</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.048</td>\n",
       "      <td>0.488</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.335</td>\n",
       "      <td>2.048</td>\n",
       "      <td>0.341</td>\n",
       "      <td>-0.917</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>TCGA-61-1743</td>\n",
       "      <td>-1.871</td>\n",
       "      <td>-1.871</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-1.037</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-1.871</td>\n",
       "      <td>0.831</td>\n",
       "      <td>-1.214</td>\n",
       "      <td>-1.214</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.871</td>\n",
       "      <td>1.559</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.607</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>TCGA-61-1907</td>\n",
       "      <td>0.401</td>\n",
       "      <td>-1.743</td>\n",
       "      <td>1.119</td>\n",
       "      <td>-1.743</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>1.232</td>\n",
       "      <td>-1.743</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.743</td>\n",
       "      <td>1.068</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>1.577</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-0.588</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>TCGA-61-1910</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-0.832</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.651</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>-0.935</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>TCGA-61-1911</td>\n",
       "      <td>0.951</td>\n",
       "      <td>-1.748</td>\n",
       "      <td>1.138</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>1.497</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>-1.748</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>1.419</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>0.849</td>\n",
       "      <td>1.831</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>TCGA-61-1914</td>\n",
       "      <td>0.288</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.682</td>\n",
       "      <td>-0.650</td>\n",
       "      <td>-0.727</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>1.077</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>-1.191</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.840</td>\n",
       "      <td>1.400</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.534</td>\n",
       "      <td>1.131</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-1.191</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-1.840</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>TCGA-61-1918</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>-1.883</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>-1.052</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.883</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>-0.719</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>TCGA-61-2000</td>\n",
       "      <td>-1.802</td>\n",
       "      <td>-1.802</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.802</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>-1.802</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-1.802</td>\n",
       "      <td>-1.156</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.802</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>-1.802</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>-0.695</td>\n",
       "      <td>-1.802</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>TCGA-61-2008</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-1.979</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-1.979</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-1.132</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-1.979</td>\n",
       "      <td>-1.979</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.517</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>-0.885</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.622</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>TCGA-61-2008</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-1.864</td>\n",
       "      <td>1.113</td>\n",
       "      <td>-0.628</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>0.992</td>\n",
       "      <td>-1.864</td>\n",
       "      <td>-1.864</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.864</td>\n",
       "      <td>1.527</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>1.378</td>\n",
       "      <td>-1.864</td>\n",
       "      <td>-1.864</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>TCGA-61-2009</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>1.078</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>-1.017</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>0.390</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>0.839</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.333</td>\n",
       "      <td>1.152</td>\n",
       "      <td>-0.543</td>\n",
       "      <td>-1.017</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>TCGA-61-2092</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>0.755</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>-1.064</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1.055</td>\n",
       "      <td>0.691</td>\n",
       "      <td>-0.665</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>TCGA-61-2094</td>\n",
       "      <td>2.523</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>-1.920</td>\n",
       "      <td>2.720</td>\n",
       "      <td>-1.920</td>\n",
       "      <td>-0.884</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.920</td>\n",
       "      <td>2.348</td>\n",
       "      <td>-0.773</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>1.843</td>\n",
       "      <td>-1.920</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.694</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>TCGA-61-2097</td>\n",
       "      <td>1.399</td>\n",
       "      <td>-1.829</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.568</td>\n",
       "      <td>-0.729</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.729</td>\n",
       "      <td>-1.187</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.829</td>\n",
       "      <td>0.257</td>\n",
       "      <td>-1.829</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.594</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>-0.729</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          patient  ENSG00000211950  ENSG00000259042  ENSG00000178372  \\\n",
       "0    TCGA-04-1331           -0.492           -1.952            0.438   \n",
       "1    TCGA-04-1332           -0.497           -0.012           -0.327   \n",
       "2    TCGA-04-1347            0.058           -1.768           -0.845   \n",
       "3    TCGA-04-1348            0.194           -1.876            0.015   \n",
       "4    TCGA-04-1362           -1.935           -1.935            0.414   \n",
       "5    TCGA-04-1364           -0.214           -1.861            0.208   \n",
       "6    TCGA-04-1365           -0.515           -0.607            0.395   \n",
       "7    TCGA-04-1514           -2.024           -2.024           -0.373   \n",
       "8    TCGA-04-1517           -0.805           -1.910           -1.910   \n",
       "9    TCGA-04-1530           -0.190           -1.997           -0.265   \n",
       "10   TCGA-04-1536            0.733           -1.681            0.096   \n",
       "11   TCGA-04-1542           -1.662           -1.662           -1.662   \n",
       "12   TCGA-04-1648           -0.252           -0.538           -0.557   \n",
       "13   TCGA-04-1651           -1.843           -0.337            0.109   \n",
       "14   TCGA-04-1655           -1.736           -1.736           -0.823   \n",
       "15   TCGA-09-0364           -0.420           -0.420           -0.625   \n",
       "16   TCGA-09-0366           -1.948           -0.909           -0.293   \n",
       "17   TCGA-09-1662           -0.639           -1.835            0.512   \n",
       "18   TCGA-09-1665            0.084           -0.216           -0.248   \n",
       "19   TCGA-09-1666           -0.550           -0.603           -0.378   \n",
       "20   TCGA-09-1667            0.322           -1.866           -0.796   \n",
       "21   TCGA-09-1670           -0.648           -1.899           -0.193   \n",
       "22   TCGA-09-2045           -1.801           -1.801           -0.011   \n",
       "23   TCGA-09-2051            0.188           -0.393           -0.002   \n",
       "24   TCGA-09-2053            0.212           -0.205           -0.418   \n",
       "25   TCGA-09-2056           -0.459           -0.244            0.311   \n",
       "26   TCGA-10-0926           -0.431           -1.863           -0.231   \n",
       "27   TCGA-10-0927            0.495           -1.695           -1.695   \n",
       "28   TCGA-10-0931           -1.770           -0.665            0.330   \n",
       "29   TCGA-10-0933           -1.805           -1.805            0.100   \n",
       "..            ...              ...              ...              ...   \n",
       "200  TCGA-31-1953           -0.300           -0.893           -0.498   \n",
       "201  TCGA-36-1568            0.217           -0.270            1.041   \n",
       "202  TCGA-36-1570           -0.643           -1.886           -0.342   \n",
       "203  TCGA-36-1571           -1.899           -1.899           -0.341   \n",
       "204  TCGA-36-1574            0.050           -0.558           -0.999   \n",
       "205  TCGA-36-1576            0.705           -0.528            0.398   \n",
       "206  TCGA-36-1578            0.266           -0.398           -1.856   \n",
       "207  TCGA-36-1580           -0.236           -0.466            0.368   \n",
       "208  TCGA-36-1581            1.091           -0.669            1.368   \n",
       "209  TCGA-57-1586           -2.026           -2.026           -0.349   \n",
       "210  TCGA-61-1725            0.846           -0.121            0.690   \n",
       "211  TCGA-61-1728           -0.767           -0.126            0.254   \n",
       "212  TCGA-61-1733           -0.410           -0.731           -1.839   \n",
       "213  TCGA-61-1736           -0.121           -1.908           -1.908   \n",
       "214  TCGA-61-1737            0.025           -0.560            0.367   \n",
       "215  TCGA-61-1738            0.319           -1.798           -1.798   \n",
       "216  TCGA-61-1741           -0.023           -2.048           -0.624   \n",
       "217  TCGA-61-1743           -1.871           -1.871           -0.274   \n",
       "218  TCGA-61-1907            0.401           -1.743            1.119   \n",
       "219  TCGA-61-1910           -1.736           -1.736           -0.194   \n",
       "220  TCGA-61-1911            0.951           -1.748            1.138   \n",
       "221  TCGA-61-1914            0.288           -0.146            0.682   \n",
       "222  TCGA-61-1918           -0.532           -1.883           -0.191   \n",
       "223  TCGA-61-2000           -1.802           -1.802            0.800   \n",
       "224  TCGA-61-2008           -0.278           -1.979            0.217   \n",
       "225  TCGA-61-2008            0.042           -1.864            1.113   \n",
       "226  TCGA-61-2009            0.111           -0.305            1.078   \n",
       "227  TCGA-61-2092           -0.135           -1.688           -1.688   \n",
       "228  TCGA-61-2094            2.523           -0.823           -0.962   \n",
       "229  TCGA-61-2097            1.399           -1.829           -0.083   \n",
       "\n",
       "     ENSG00000120937  ENSG00000150750  ENSG00000156076  ENSG00000211947  \\\n",
       "0             -0.988           -0.909           -0.755            0.296   \n",
       "1             -0.281           -1.083           -0.043            0.048   \n",
       "2             -0.770           -1.768           -1.124            0.924   \n",
       "3             -1.876           -1.037           -0.930            0.627   \n",
       "4             -1.935           -0.668           -0.892           -0.831   \n",
       "5             -1.861           -0.039           -1.861           -1.861   \n",
       "6             -0.855           -1.039           -0.855            0.569   \n",
       "7             -0.774           -0.545            0.049           -0.905   \n",
       "8             -0.754           -1.236           -0.380            0.051   \n",
       "9             -0.589           -1.997           -0.451            0.791   \n",
       "10            -1.681           -1.681           -1.681            0.057   \n",
       "11            -1.662           -1.662           -0.767           -0.412   \n",
       "12            -0.792            0.245           -0.792            0.196   \n",
       "13            -0.415           -0.294           -1.843           -0.114   \n",
       "14            -1.736           -1.736           -0.461           -0.602   \n",
       "15            -0.953           -0.434           -0.765           -0.723   \n",
       "16            -1.096           -1.948           -1.948            0.123   \n",
       "17            -1.835           -0.900           -0.824           -0.225   \n",
       "18            -1.823           -1.823           -0.592            0.659   \n",
       "19            -1.811           -1.811           -0.669           -0.241   \n",
       "20            -0.747           -1.866           -1.866            1.211   \n",
       "21            -1.899           -0.717           -0.390            0.481   \n",
       "22            -1.801           -0.221           -1.801            0.168   \n",
       "23            -1.911           -1.911           -0.571            0.472   \n",
       "24            -1.974           -0.808           -0.499            1.328   \n",
       "25            -1.738           -1.099           -0.566            0.820   \n",
       "26            -0.274           -0.293            0.160           -0.143   \n",
       "27            -1.695           -1.695           -0.579            0.246   \n",
       "28            -0.665            0.070           -1.770            0.105   \n",
       "29            -0.419           -1.805           -0.074           -1.805   \n",
       "..               ...              ...              ...              ...   \n",
       "200           -0.454           -1.920           -1.920            0.024   \n",
       "201           -1.862           -0.920           -0.064            0.260   \n",
       "202           -1.047            0.073           -1.225           -0.939   \n",
       "203           -1.899           -0.453           -0.866           -0.866   \n",
       "204           -1.805           -1.805           -1.805            0.955   \n",
       "205           -0.528           -0.700           -1.911            1.135   \n",
       "206           -1.856           -1.856           -1.197            1.967   \n",
       "207           -1.949           -1.272           -0.901            0.442   \n",
       "208           -0.619           -1.002           -1.111            1.060   \n",
       "209           -2.026           -0.349           -0.396            0.967   \n",
       "210           -0.787           -1.783           -0.347            1.221   \n",
       "211           -0.809           -0.351           -1.967            0.176   \n",
       "212           -0.690           -1.839           -1.839           -0.062   \n",
       "213           -0.766           -0.766           -0.654            0.231   \n",
       "214           -1.865           -1.865           -1.865            0.429   \n",
       "215           -1.798           -0.985           -0.985            1.250   \n",
       "216           -2.048           -0.509           -0.238            1.703   \n",
       "217           -1.037           -0.930           -1.871            0.831   \n",
       "218           -1.743           -0.036           -0.442            1.232   \n",
       "219           -1.736           -1.736           -1.736           -1.736   \n",
       "220           -0.630           -0.490           -0.450            1.497   \n",
       "221           -0.650           -0.727           -0.378            1.077   \n",
       "222           -0.869           -0.869           -0.869           -0.246   \n",
       "223           -1.802           -0.451           -1.802            0.021   \n",
       "224           -1.979           -0.233           -1.132            0.104   \n",
       "225           -0.628           -0.058           -0.340            0.992   \n",
       "226           -1.843           -1.017           -1.843            0.390   \n",
       "227           -1.688           -0.440           -1.688            0.755   \n",
       "228           -0.730           -0.823           -1.920            2.720   \n",
       "229           -0.568           -0.729           -0.396            0.007   \n",
       "\n",
       "     ENSG00000091583  ENSG00000254951  ...  ENSG00000185966  ENSG00000211659  \\\n",
       "0             -1.952           -0.508  ...           -1.952            0.643   \n",
       "1             -0.419           -0.497  ...           -1.959            0.178   \n",
       "2             -0.770           -1.768  ...           -1.768            0.667   \n",
       "3             -1.037           -1.876  ...           -1.876            0.268   \n",
       "4             -0.526           -1.935  ...           -0.613           -0.781   \n",
       "5             -1.031           -1.208  ...           -0.512           -1.861   \n",
       "6             -1.039           -1.877  ...           -1.877            1.597   \n",
       "7             -1.158           -2.024  ...           -2.024           -2.024   \n",
       "8             -1.054           -1.910  ...           -1.910           -0.288   \n",
       "9             -1.997           -1.997  ...           -0.861            1.103   \n",
       "10            -1.681           -0.600  ...           -0.495            0.019   \n",
       "11            -0.099           -0.590  ...           -1.662           -1.662   \n",
       "12            -0.981           -1.270  ...           -1.941            0.076   \n",
       "13            -1.843           -0.823  ...           -1.843            1.012   \n",
       "14            -0.369           -0.748  ...           -1.736           -0.193   \n",
       "15            -0.249           -0.271  ...           -1.908           -1.908   \n",
       "16            -0.987           -1.096  ...           -1.948            0.084   \n",
       "17            -1.006           -1.182  ...           -1.835            0.102   \n",
       "18            -1.823           -1.168  ...           -0.592            0.666   \n",
       "19            -1.811           -0.505  ...           -1.811           -0.419   \n",
       "20            -0.483           -1.214  ...           -1.866            0.932   \n",
       "21            -0.443           -0.648  ...           -1.899           -0.010   \n",
       "22            -1.801           -0.880  ...           -1.801            0.033   \n",
       "23            -0.704           -1.911  ...           -1.911            0.502   \n",
       "24            -1.974           -1.000  ...           -1.974            1.490   \n",
       "25            -0.642           -1.099  ...           -1.738            0.359   \n",
       "26            -1.863           -1.015  ...           -0.829            0.266   \n",
       "27            -0.259           -0.078  ...           -1.695            0.603   \n",
       "28            -1.770           -0.440  ...           -0.952           -0.105   \n",
       "29            -0.869           -0.575  ...           -1.805           -1.805   \n",
       "..               ...              ...  ...              ...              ...   \n",
       "200           -1.920           -0.705  ...           -1.920            0.478   \n",
       "201           -1.862           -1.862  ...           -1.862            0.364   \n",
       "202           -1.886           -0.643  ...           -0.802           -0.503   \n",
       "203           -0.756           -1.899  ...           -1.899           -0.589   \n",
       "204           -0.764           -1.805  ...           -1.805            0.762   \n",
       "205           -1.911           -1.250  ...           -1.911            0.902   \n",
       "206           -0.306           -0.835  ...           -1.856            0.983   \n",
       "207           -0.619           -0.980  ...           -1.949            1.162   \n",
       "208           -1.967           -1.293  ...           -1.967            1.321   \n",
       "209           -2.026           -0.640  ...           -0.212           -0.406   \n",
       "210           -0.007           -1.783  ...           -1.783            0.757   \n",
       "211           -1.110           -0.697  ...           -1.967            0.298   \n",
       "212           -1.839           -0.690  ...           -1.839            0.450   \n",
       "213           -1.908           -0.723  ...           -1.908            0.128   \n",
       "214           -1.865           -1.865  ...           -1.865            0.660   \n",
       "215           -0.075           -1.798  ...           -1.798            1.207   \n",
       "216           -1.060           -1.358  ...           -2.048            0.488   \n",
       "217           -1.214           -1.214  ...           -1.871            1.559   \n",
       "218           -1.743           -0.349  ...           -1.743            1.068   \n",
       "219           -0.272           -0.832  ...           -1.736           -1.736   \n",
       "220           -0.630           -1.748  ...           -0.399            1.419   \n",
       "221           -0.541           -1.191  ...           -1.840            1.400   \n",
       "222           -0.435           -1.052  ...           -1.883            0.039   \n",
       "223           -1.802           -1.156  ...           -1.802           -0.304   \n",
       "224           -1.979           -1.979  ...           -0.298           -0.080   \n",
       "225           -1.864           -1.864  ...           -1.864            1.527   \n",
       "226           -1.843           -0.912  ...           -1.843            0.839   \n",
       "227           -1.688           -1.064  ...           -1.688            0.688   \n",
       "228           -1.920           -0.884  ...           -1.920            2.348   \n",
       "229           -0.729           -1.187  ...           -1.829            0.257   \n",
       "\n",
       "     ENSG00000187492  ENSG00000166415  ENSG00000244437  ENSG00000064218  \\\n",
       "0              0.206           -1.097            0.413           -0.170   \n",
       "1             -1.269           -0.118            0.359           -1.959   \n",
       "2             -0.622            0.531            1.172           -1.768   \n",
       "3             -0.744            0.616            1.533           -0.853   \n",
       "4             -0.168            0.718           -0.362           -0.306   \n",
       "5              0.474            0.516           -0.288           -1.861   \n",
       "6             -0.418            0.783            1.266           -0.326   \n",
       "7             -0.213            0.438           -0.711            0.282   \n",
       "8             -0.205            0.118            0.271           -0.481   \n",
       "9             -0.379            0.233            0.854           -0.096   \n",
       "10            -1.681           -0.420            0.209           -1.050   \n",
       "11            -1.662           -0.767           -1.662           -0.868   \n",
       "12             0.491            0.267            0.449           -0.680   \n",
       "13            -1.843           -0.068           -0.276            0.234   \n",
       "14            -1.736           -0.156           -0.013           -1.736   \n",
       "15            -0.407            0.893           -1.908           -1.908   \n",
       "16            -0.163            0.959            0.185           -1.948   \n",
       "17            -1.835           -0.824            0.527           -0.716   \n",
       "18            -1.823            0.824            0.844            0.054   \n",
       "19            -0.282            0.414            0.137           -0.816   \n",
       "20            -1.214           -1.866            1.355           -0.437   \n",
       "21            -0.165            0.119            0.176           -0.259   \n",
       "22             0.165           -0.253            0.471           -0.308   \n",
       "23            -1.074            0.681            0.712           -0.831   \n",
       "24            -0.267           -0.443            1.583           -1.110   \n",
       "25            -1.738           -1.738            0.566           -1.738   \n",
       "26            -0.325            0.456            0.520           -0.578   \n",
       "27            -0.440           -0.545            0.781           -1.695   \n",
       "28            -0.099            0.129            0.249           -1.126   \n",
       "29            -0.475            0.584           -1.805            0.345   \n",
       "..               ...              ...              ...              ...   \n",
       "200            0.378            0.684            0.525           -0.428   \n",
       "201           -1.026            0.020            0.584           -1.204   \n",
       "202            0.973            0.863           -0.190           -1.886   \n",
       "203            0.509            0.857           -0.321            0.008   \n",
       "204           -0.277           -0.253            0.761           -0.514   \n",
       "205           -0.423            0.559            1.859           -0.411   \n",
       "206           -1.856           -0.070            1.533           -0.775   \n",
       "207           -0.901            0.073            0.667           -0.646   \n",
       "208           -1.002            0.790            1.681           -0.862   \n",
       "209            0.265            0.661            1.258           -1.146   \n",
       "210           -0.203           -0.389            1.389            0.073   \n",
       "211            0.123            0.545            0.599           -1.110   \n",
       "212           -0.329            0.573            0.346           -0.623   \n",
       "213            0.668           -0.229            0.482           -1.908   \n",
       "214           -0.248           -0.705            0.718           -0.129   \n",
       "215           -0.495           -0.291            1.322           -0.659   \n",
       "216           -0.406            0.335            2.048            0.341   \n",
       "217           -0.930            0.317            0.195           -0.607   \n",
       "218            0.283           -0.234            1.577           -0.312   \n",
       "219           -0.383            0.651           -1.736           -0.935   \n",
       "220           -0.774            0.849            1.831           -0.469   \n",
       "221           -0.112            0.534            1.131           -0.224   \n",
       "222           -0.385           -0.307            0.047           -0.623   \n",
       "223           -1.802            0.779            0.141           -0.314   \n",
       "224           -0.051           -0.517            0.173           -0.532   \n",
       "225            0.062           -0.508            1.378           -1.864   \n",
       "226           -0.356            0.333            1.152           -0.543   \n",
       "227            0.180            0.775            1.055            0.691   \n",
       "228           -0.773           -0.318            1.843           -1.920   \n",
       "229           -1.829            0.453            0.038           -0.594   \n",
       "\n",
       "     ENSG00000243775  ENSG00000188817  ENSG00000211818  Platinum_Status  \n",
       "0             -0.798           -0.061           -0.255                0  \n",
       "1             -1.959           -0.557           -0.018                0  \n",
       "2             -0.950           -1.768           -1.768                0  \n",
       "3             -0.853           -0.744           -0.064                0  \n",
       "4             -1.262           -0.179           -0.701                1  \n",
       "5             -0.741            0.398           -1.861                1  \n",
       "6             -1.217           -0.418           -0.704                0  \n",
       "7             -1.341           -0.204           -2.024                0  \n",
       "8             -1.236           -0.368           -1.910                0  \n",
       "9             -1.997           -0.401           -0.401                0  \n",
       "10            -0.880           -1.681           -1.681                0  \n",
       "11            -0.868           -0.201           -1.662                0  \n",
       "12            -1.090            0.602           -0.311                0  \n",
       "13            -1.185           -1.843           -1.843                0  \n",
       "14            -1.736           -0.509           -1.736                0  \n",
       "15            -0.953           -0.256           -0.625                0  \n",
       "16            -0.909            0.114           -0.630                1  \n",
       "17            -0.675           -1.835           -0.291                0  \n",
       "18            -1.823           -1.823           -1.823                0  \n",
       "19            -1.811           -0.125           -0.467                0  \n",
       "20            -1.038           -0.856           -0.079                0  \n",
       "21            -1.234           -0.217           -0.759                0  \n",
       "22            -0.880            0.184           -1.801                0  \n",
       "23            -1.252           -0.551           -0.643                0  \n",
       "24            -1.000           -0.456           -1.974                0  \n",
       "25            -1.738           -0.535           -0.459                0  \n",
       "26            -1.863           -0.086           -1.863                1  \n",
       "27            -1.695           -0.462           -1.695                0  \n",
       "28            -1.126            0.009           -0.772                1  \n",
       "29            -1.805           -0.521           -1.805                0  \n",
       "..               ...              ...              ...              ...  \n",
       "200           -1.257            0.081           -0.300                0  \n",
       "201           -0.920           -0.843           -0.145                0  \n",
       "202           -1.225            1.008           -1.886                1  \n",
       "203           -0.615            0.486           -0.225                1  \n",
       "204           -0.896            0.150           -0.428                0  \n",
       "205           -1.911           -0.304           -0.086                1  \n",
       "206           -1.197           -0.537           -0.537                0  \n",
       "207           -0.839           -0.533           -0.746                0  \n",
       "208           -0.731           -0.402           -0.203                1  \n",
       "209           -0.617            0.470           -0.756                0  \n",
       "210           -0.967           -0.138           -0.243                0  \n",
       "211           -1.292            0.249           -0.860                1  \n",
       "212           -1.019           -0.238           -0.623                0  \n",
       "213           -0.766            0.539           -1.908                0  \n",
       "214           -0.746           -0.483           -0.466                0  \n",
       "215           -0.699           -0.457           -1.798                0  \n",
       "216           -0.917           -0.222           -0.437                0  \n",
       "217           -0.930           -0.516           -0.516                0  \n",
       "218           -0.588            0.250           -0.103                0  \n",
       "219           -0.434           -0.416           -1.736                1  \n",
       "220           -0.774           -0.022           -0.370                0  \n",
       "221           -1.191           -0.146           -1.840                0  \n",
       "222           -0.719           -0.191           -0.316                0  \n",
       "223           -0.695           -1.802           -0.434                0  \n",
       "224           -0.885            0.021           -0.622                0  \n",
       "225           -1.864            0.003           -0.275                0  \n",
       "226           -1.017           -0.356           -0.345                0  \n",
       "227           -0.665            0.080           -1.688                1  \n",
       "228           -0.962           -0.694           -0.001                0  \n",
       "229           -0.910           -0.729           -0.469                0  \n",
       "\n",
       "[230 rows x 202 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Train Data - for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information of Training Data\n",
      "Samples : 230\n",
      "Features : 200\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 과정\n",
    "# \"patient\" 컬럼 (축의 내용을 설명하는 부분) 을 제거하고, Input과 Target Data를 분리함\n",
    "x = data.drop([\"patient\",\"Platinum_Status\"],axis=1).values\n",
    "y = data.Platinum_Status.values\n",
    "\n",
    "# 샘플 수와 피쳐 수 출력해보기\n",
    "print(\"Information of Training Data\")\n",
    "print(\"Samples : {}\".format(x.shape[0]))\n",
    "print(\"Features : {}\".format(x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test Dataset 나누어주기\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Validation Dataset 나누어주기\n",
    "train_x, val_x, train_y, val_y = train_test_split(x, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f4dad7e16a03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model = tf.keras.models.Sequential([\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'zeros'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ones'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#tf.keras.layers.Dropout(0.5),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "model = tf.keras.models.Sequential([\n",
    "    \n",
    "    # Densely Connected Layer (Input: 200D, Output: 100D)\n",
    "    tf.keras.layers.Dense(100, input_shape=(200,),activation='relu'),\n",
    "    # Batch Normalization Layer\n",
    "    tf.keras.layers.BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones'),\n",
    "    \n",
    "    # Densely Connected Layer (Input: 100D, Output: 80D)\n",
    "    tf.keras.layers.Dense(80, input_shape=(100,),activation='relu'), \n",
    "    # Batch Normalization Layer\n",
    "    tf.keras.layers.BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones'),\n",
    "    \n",
    "    # Densely Connected Layer (Input: 80D, Output: 60D)\n",
    "    tf.keras.layers.Dense(60, input_shape=(80,),activation='relu'), \n",
    "    # Batch Normalization Layer\n",
    "    tf.keras.layers.BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones'),\n",
    "    \n",
    "    # Densely Connected Layer (Input: 60D, Output: 40D)\n",
    "    tf.keras.layers.Dense(40, input_shape=(60,),activation='relu'), \n",
    "    # Batch Normalization Layer\n",
    "    tf.keras.layers.BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones'),\n",
    "    \n",
    "    # Densely Connected Layer (Input: 40D, Output: 20D)\n",
    "    tf.keras.layers.Dense(20, input_shape=(40,),activation='relu'), \n",
    "    # Batch Normalization Layer\n",
    "    tf.keras.layers.BatchNormalization(axis=1, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones'),\n",
    "    \n",
    "    # Densely Connected Layer (Input: 20D, Output: 1D)\n",
    "    tf.keras.layers.Dense(1, input_shape=(20,),activation='sigmoid'),\n",
    "])\n",
    "\n",
    "# 모델 학습에 필요한 정보들을 명시 (Optimizer, Loss Function, Metric 등)\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "              loss=custom_loss,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 1s 3ms/sample - loss: 0.4876 - acc: 0.4239\n",
      "184/184 [==============================] - 0s 103us/sample - loss: 0.4876 - acc: 0.4239\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.4876 - acc: 0.4239\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.4967 - acc: 0.3478\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.4967 - acc: 0.3478\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4967 - acc: 0.3478\n",
      "Training Sensitivity :  0.84\n",
      "Training Specificity :  0.26865671641791045\n",
      "Validation Sensitivity :  0.7222222222222222\n",
      "Validation Specificity :  0.10714285714285714\n",
      "Current training Accuracy : 0.42391303\n",
      "Current validation Accuracy : 0.3478261\n",
      "Current training loss : 0.48756478402925574\n",
      "Current validation loss : 0.49674459643985913\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.4827 - acc: 0.4402\n",
      "184/184 [==============================] - 0s 98us/sample - loss: 0.4827 - acc: 0.4402\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.4827 - acc: 0.4402\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.4983 - acc: 0.4565\n",
      "46/46 [==============================] - 0s 195us/sample - loss: 0.4983 - acc: 0.4565\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4983 - acc: 0.4565\n",
      "Training Sensitivity :  0.84\n",
      "Training Specificity :  0.291044776119403\n",
      "Validation Sensitivity :  0.7777777777777778\n",
      "Validation Specificity :  0.25\n",
      "Current training Accuracy : 0.4402174\n",
      "Current validation Accuracy : 0.45652175\n",
      "Current training loss : 0.4827267242514569\n",
      "Current validation loss : 0.49834685999414197\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.4805 - acc: 0.5054\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.4805 - acc: 0.5054\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.4805 - acc: 0.5054\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4972 - acc: 0.5217\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4972 - acc: 0.5217\n",
      "46/46 [==============================] - 0s 195us/sample - loss: 0.4972 - acc: 0.5217\n",
      "Training Sensitivity :  0.76\n",
      "Training Specificity :  0.41044776119402987\n",
      "Validation Sensitivity :  0.6111111111111112\n",
      "Validation Specificity :  0.4642857142857143\n",
      "Current training Accuracy : 0.5054348\n",
      "Current validation Accuracy : 0.5217391\n",
      "Current training loss : 0.48051259310349176\n",
      "Current validation loss : 0.4971895632536515\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.4766 - acc: 0.5815\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.4766 - acc: 0.5815\n",
      "184/184 [==============================] - 0s 98us/sample - loss: 0.4766 - acc: 0.5815\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.5006 - acc: 0.5000\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.5006 - acc: 0.5000\n",
      "46/46 [==============================] - 0s 195us/sample - loss: 0.5006 - acc: 0.5000\n",
      "Training Sensitivity :  0.72\n",
      "Training Specificity :  0.5298507462686567\n",
      "Validation Sensitivity :  0.5555555555555556\n",
      "Validation Specificity :  0.4642857142857143\n",
      "Current training Accuracy : 0.58152175\n",
      "Current validation Accuracy : 0.5\n",
      "Current training loss : 0.4765659441118655\n",
      "Current validation loss : 0.5005926412084828\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.4740 - acc: 0.5815\n",
      "184/184 [==============================] - 0s 114us/sample - loss: 0.4740 - acc: 0.5815\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.4740 - acc: 0.5815\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4938 - acc: 0.5217\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4938 - acc: 0.5217\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4938 - acc: 0.5217\n",
      "Training Sensitivity :  0.54\n",
      "Training Specificity :  0.5970149253731343\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.5357142857142857\n",
      "Current training Accuracy : 0.58152175\n",
      "Current validation Accuracy : 0.5217391\n",
      "Current training loss : 0.47397761241249414\n",
      "Current validation loss : 0.49381427920382953\n",
      "184/184 [==============================] - 0s 98us/sample - loss: 0.4679 - acc: 0.6467\n",
      "184/184 [==============================] - 0s 108us/sample - loss: 0.4679 - acc: 0.6467\n",
      "184/184 [==============================] - 0s 114us/sample - loss: 0.4679 - acc: 0.6467\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4803 - acc: 0.6087\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4803 - acc: 0.6087\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4803 - acc: 0.6087\n",
      "Training Sensitivity :  0.42\n",
      "Training Specificity :  0.7313432835820896\n",
      "Validation Sensitivity :  0.5555555555555556\n",
      "Validation Specificity :  0.6428571428571429\n",
      "Current training Accuracy : 0.6467391\n",
      "Current validation Accuracy : 0.6086956\n",
      "Current training loss : 0.4679281867068747\n",
      "Current validation loss : 0.4802595273308132\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.4571 - acc: 0.6957\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.4571 - acc: 0.6957\n",
      "184/184 [==============================] - 0s 108us/sample - loss: 0.4571 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4786 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4786 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.4786 - acc: 0.6304\n",
      "Training Sensitivity :  0.4\n",
      "Training Specificity :  0.8059701492537313\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.75\n",
      "Current training Accuracy : 0.6956522\n",
      "Current validation Accuracy : 0.6304348\n",
      "Current training loss : 0.45708690518918244\n",
      "Current validation loss : 0.4785978586777397\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.4527 - acc: 0.7011\n",
      "184/184 [==============================] - 0s 98us/sample - loss: 0.4527 - acc: 0.7011\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.4527 - acc: 0.7011\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4735 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.4735 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.4735 - acc: 0.6304\n",
      "Training Sensitivity :  0.4\n",
      "Training Specificity :  0.8134328358208955\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.7142857142857143\n",
      "Current training Accuracy : 0.70108694\n",
      "Current validation Accuracy : 0.6304348\n",
      "Current training loss : 0.4526877144108648\n",
      "Current validation loss : 0.473527351151342\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.4612 - acc: 0.7174\n",
      "184/184 [==============================] - 0s 108us/sample - loss: 0.4612 - acc: 0.7174\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.4612 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4732 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4732 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4732 - acc: 0.6304\n",
      "Training Sensitivity :  0.28\n",
      "Training Specificity :  0.8805970149253731\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.75\n",
      "Current training Accuracy : 0.7173913\n",
      "Current validation Accuracy : 0.6304348\n",
      "Current training loss : 0.46118001834205957\n",
      "Current validation loss : 0.4731862052627232\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.4590 - acc: 0.6957\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.4590 - acc: 0.6957\n",
      "184/184 [==============================] - 0s 103us/sample - loss: 0.4590 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4708 - acc: 0.6087\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4708 - acc: 0.6087\n",
      "46/46 [==============================] - 0s 195us/sample - loss: 0.4708 - acc: 0.6087\n",
      "Training Sensitivity :  0.38\n",
      "Training Specificity :  0.8134328358208955\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.75\n",
      "Current training Accuracy : 0.6956522\n",
      "Current validation Accuracy : 0.6086956\n",
      "Current training loss : 0.45902921842492145\n",
      "Current validation loss : 0.47080150894496753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 0s 65us/sample - loss: 0.4404 - acc: 0.6902\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.4404 - acc: 0.6902\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.4404 - acc: 0.6902\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.4838 - acc: 0.4783\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4838 - acc: 0.4783\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4838 - acc: 0.4783\n",
      "Training Sensitivity :  0.62\n",
      "Training Specificity :  0.7164179104477612\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.5357142857142857\n",
      "Current training Accuracy : 0.6902174\n",
      "Current validation Accuracy : 0.47826087\n",
      "Current training loss : 0.44044430359550146\n",
      "Current validation loss : 0.48384886720906134\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.4451 - acc: 0.6685\n",
      "184/184 [==============================] - 0s 103us/sample - loss: 0.4451 - acc: 0.6685\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.4451 - acc: 0.6685\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4767 - acc: 0.5870\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.4767 - acc: 0.5870\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4767 - acc: 0.5870\n",
      "Training Sensitivity :  0.5\n",
      "Training Specificity :  0.7313432835820896\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.6428571428571429\n",
      "Current training Accuracy : 0.66847825\n",
      "Current validation Accuracy : 0.5869565\n",
      "Current training loss : 0.44507218443829083\n",
      "Current validation loss : 0.476666289827098\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.4337 - acc: 0.6957\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.4337 - acc: 0.6957\n",
      "184/184 [==============================] - 0s 98us/sample - loss: 0.4337 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4625 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 195us/sample - loss: 0.4625 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4625 - acc: 0.6522\n",
      "Training Sensitivity :  0.56\n",
      "Training Specificity :  0.746268656716418\n",
      "Validation Sensitivity :  0.6111111111111112\n",
      "Validation Specificity :  0.6785714285714286\n",
      "Current training Accuracy : 0.6956522\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.4336656435676243\n",
      "Current validation loss : 0.4625235459078913\n",
      "184/184 [==============================] - 0s 98us/sample - loss: 0.4155 - acc: 0.7011\n",
      "184/184 [==============================] - 0s 98us/sample - loss: 0.4155 - acc: 0.7011\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.4155 - acc: 0.7011\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4575 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.4575 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4575 - acc: 0.6739\n",
      "Training Sensitivity :  0.56\n",
      "Training Specificity :  0.753731343283582\n",
      "Validation Sensitivity :  0.6111111111111112\n",
      "Validation Specificity :  0.7142857142857143\n",
      "Current training Accuracy : 0.70108694\n",
      "Current validation Accuracy : 0.67391306\n",
      "Current training loss : 0.41551392752191296\n",
      "Current validation loss : 0.45745520488075586\n",
      "184/184 [==============================] - 0s 70us/sample - loss: 0.3864 - acc: 0.6957\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3864 - acc: 0.6957\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3864 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4369 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4369 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4369 - acc: 0.6739\n",
      "Training Sensitivity :  0.58\n",
      "Training Specificity :  0.7388059701492538\n",
      "Validation Sensitivity :  0.6111111111111112\n",
      "Validation Specificity :  0.7142857142857143\n",
      "Current training Accuracy : 0.6956522\n",
      "Current validation Accuracy : 0.67391306\n",
      "Current training loss : 0.38641084277111554\n",
      "Current validation loss : 0.43691047378208325\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3777 - acc: 0.7065\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3777 - acc: 0.7065\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3777 - acc: 0.7065\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4341 - acc: 0.6087\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.4341 - acc: 0.6087\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4341 - acc: 0.6087\n",
      "Training Sensitivity :  0.58\n",
      "Training Specificity :  0.753731343283582\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.7142857142857143\n",
      "Current training Accuracy : 0.70652175\n",
      "Current validation Accuracy : 0.6086956\n",
      "Current training loss : 0.3776566438052965\n",
      "Current validation loss : 0.4340600630511408\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.3676 - acc: 0.7826\n",
      "184/184 [==============================] - 0s 98us/sample - loss: 0.3676 - acc: 0.7826\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.3676 - acc: 0.7826\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4344 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.4344 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4344 - acc: 0.6522\n",
      "Training Sensitivity :  0.56\n",
      "Training Specificity :  0.8656716417910447\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.7857142857142857\n",
      "Current training Accuracy : 0.7826087\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.36758540246797644\n",
      "Current validation loss : 0.43440453902534815\n",
      "184/184 [==============================] - 0s 70us/sample - loss: 0.3676 - acc: 0.8207\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3676 - acc: 0.8207\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.3676 - acc: 0.8207\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4319 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4319 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4319 - acc: 0.6739\n",
      "Training Sensitivity :  0.58\n",
      "Training Specificity :  0.9104477611940298\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.8571428571428571\n",
      "Current training Accuracy : 0.8206522\n",
      "Current validation Accuracy : 0.67391306\n",
      "Current training loss : 0.36757940831391706\n",
      "Current validation loss : 0.4318848641022392\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.3608 - acc: 0.8043\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3608 - acc: 0.8043\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.3608 - acc: 0.8043\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4222 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4222 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4222 - acc: 0.6739\n",
      "Training Sensitivity :  0.56\n",
      "Training Specificity :  0.8955223880597015\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.8571428571428571\n",
      "Current training Accuracy : 0.8043478\n",
      "Current validation Accuracy : 0.67391306\n",
      "Current training loss : 0.36082427398018213\n",
      "Current validation loss : 0.4222303317940753\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.3454 - acc: 0.8098\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.3454 - acc: 0.8098\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.3454 - acc: 0.8098\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4141 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4141 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4141 - acc: 0.6739\n",
      "Training Sensitivity :  0.58\n",
      "Training Specificity :  0.8955223880597015\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.8571428571428571\n",
      "Current training Accuracy : 0.8097826\n",
      "Current validation Accuracy : 0.67391306\n",
      "Current training loss : 0.34544781498287036\n",
      "Current validation loss : 0.4141286870707636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 0s 76us/sample - loss: 0.3430 - acc: 0.7717\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.3430 - acc: 0.7717\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.3430 - acc: 0.7717\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4059 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4059 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4059 - acc: 0.6522\n",
      "Training Sensitivity :  0.66\n",
      "Training Specificity :  0.8134328358208955\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.7857142857142857\n",
      "Current training Accuracy : 0.7717391\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.3429948609808217\n",
      "Current validation loss : 0.4058815396350363\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.3331 - acc: 0.7554\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3331 - acc: 0.7554\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3331 - acc: 0.7554\n",
      "46/46 [==============================] - 0s 174us/sample - loss: 0.4043 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4043 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4043 - acc: 0.6522\n",
      "Training Sensitivity :  0.68\n",
      "Training Specificity :  0.7835820895522388\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.7857142857142857\n",
      "Current training Accuracy : 0.7554348\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.33310126480848895\n",
      "Current validation loss : 0.40429081605828326\n",
      "184/184 [==============================] - 0s 152us/sample - loss: 0.3330 - acc: 0.7446\n",
      "184/184 [==============================] - 0s 70us/sample - loss: 0.3330 - acc: 0.7446\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.3330 - acc: 0.7446\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4028 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4028 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4028 - acc: 0.6739\n",
      "Training Sensitivity :  0.66\n",
      "Training Specificity :  0.7761194029850746\n",
      "Validation Sensitivity :  0.5555555555555556\n",
      "Validation Specificity :  0.75\n",
      "Current training Accuracy : 0.7445652\n",
      "Current validation Accuracy : 0.67391306\n",
      "Current training loss : 0.3330143845599631\n",
      "Current validation loss : 0.4028113976768825\n",
      "184/184 [==============================] - 0s 114us/sample - loss: 0.3309 - acc: 0.7500\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.3309 - acc: 0.7500\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.3309 - acc: 0.7500\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4054 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4054 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4054 - acc: 0.6522\n",
      "Training Sensitivity :  0.66\n",
      "Training Specificity :  0.7835820895522388\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.75\n",
      "Current training Accuracy : 0.75\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.3309433667556099\n",
      "Current validation loss : 0.4054447671641474\n",
      "184/184 [==============================] - 0s 98us/sample - loss: 0.3263 - acc: 0.7826\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.3263 - acc: 0.7826\n",
      "184/184 [==============================] - 0s 103us/sample - loss: 0.3263 - acc: 0.7826\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4210 - acc: 0.5870\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4210 - acc: 0.5870\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4210 - acc: 0.5870\n",
      "Training Sensitivity :  0.58\n",
      "Training Specificity :  0.8582089552238806\n",
      "Validation Sensitivity :  0.2777777777777778\n",
      "Validation Specificity :  0.7857142857142857\n",
      "Current training Accuracy : 0.7826087\n",
      "Current validation Accuracy : 0.5869565\n",
      "Current training loss : 0.32627107267794403\n",
      "Current validation loss : 0.42097471071326215\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.3248 - acc: 0.7717\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3248 - acc: 0.7717\n",
      "184/184 [==============================] - 0s 98us/sample - loss: 0.3248 - acc: 0.7717\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4270 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4270 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4270 - acc: 0.6522\n",
      "Training Sensitivity :  0.6\n",
      "Training Specificity :  0.835820895522388\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.7857142857142857\n",
      "Current training Accuracy : 0.7717391\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.32476776060850726\n",
      "Current validation loss : 0.42695125548735907\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.3263 - acc: 0.7663\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3263 - acc: 0.7663\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3263 - acc: 0.7663\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4356 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4356 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4356 - acc: 0.6522\n",
      "Training Sensitivity :  0.58\n",
      "Training Specificity :  0.835820895522388\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.7663044\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.32634462999260944\n",
      "Current validation loss : 0.43560817189838574\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3244 - acc: 0.7717\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.3244 - acc: 0.7717\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.3244 - acc: 0.7717\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4447 - acc: 0.5870\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4447 - acc: 0.5870\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4447 - acc: 0.5870\n",
      "Training Sensitivity :  0.56\n",
      "Training Specificity :  0.8507462686567164\n",
      "Validation Sensitivity :  0.2222222222222222\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.7717391\n",
      "Current validation Accuracy : 0.5869565\n",
      "Current training loss : 0.32444306560184644\n",
      "Current validation loss : 0.44469257023023523\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.3177 - acc: 0.7772\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3177 - acc: 0.7772\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.3177 - acc: 0.7772\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4372 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.4372 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.4372 - acc: 0.6522\n",
      "Training Sensitivity :  0.56\n",
      "Training Specificity :  0.8582089552238806\n",
      "Validation Sensitivity :  0.3333333333333333\n",
      "Validation Specificity :  0.8571428571428571\n",
      "Current training Accuracy : 0.77717394\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.3176710968432219\n",
      "Current validation loss : 0.4371831883554873\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.3190 - acc: 0.7663\n",
      "184/184 [==============================] - 0s 70us/sample - loss: 0.3190 - acc: 0.7663\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3190 - acc: 0.7663\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4422 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4422 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4422 - acc: 0.6522\n",
      "Training Sensitivity :  0.56\n",
      "Training Specificity :  0.8432835820895522\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.7663044\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.3190124967823858\n",
      "Current validation loss : 0.44221785016681836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3166 - acc: 0.7554\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.3166 - acc: 0.7554\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3166 - acc: 0.7554\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4247 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4247 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4247 - acc: 0.6739\n",
      "Training Sensitivity :  0.54\n",
      "Training Specificity :  0.835820895522388\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.8571428571428571\n",
      "Current training Accuracy : 0.7554348\n",
      "Current validation Accuracy : 0.67391306\n",
      "Current training loss : 0.3165817208912062\n",
      "Current validation loss : 0.42465248574381287\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.3062 - acc: 0.7554\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.3062 - acc: 0.7554\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.3062 - acc: 0.7554\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.4094 - acc: 0.6957\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3886 - acc: 0.750 - 0s 173us/sample - loss: 0.4094 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4094 - acc: 0.6957\n",
      "Training Sensitivity :  0.58\n",
      "Training Specificity :  0.8208955223880597\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.7554348\n",
      "Current validation Accuracy : 0.6956522\n",
      "Current training loss : 0.3062095590259718\n",
      "Current validation loss : 0.40943749313769134\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.3020 - acc: 0.7609\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.3020 - acc: 0.7609\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.3020 - acc: 0.7609\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4098 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4098 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4098 - acc: 0.6522\n",
      "Training Sensitivity :  0.62\n",
      "Training Specificity :  0.8134328358208955\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.75\n",
      "Current training Accuracy : 0.76086956\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.301989627921063\n",
      "Current validation loss : 0.4098008611927862\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.3008 - acc: 0.7826\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.3008 - acc: 0.7826\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.3008 - acc: 0.7826\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3988 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.3988 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3988 - acc: 0.6957\n",
      "Training Sensitivity :  0.56\n",
      "Training Specificity :  0.8656716417910447\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.7826087\n",
      "Current validation Accuracy : 0.6956522\n",
      "Current training loss : 0.3007715370344079\n",
      "Current validation loss : 0.39878933844359027\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.2859 - acc: 0.7717\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.2859 - acc: 0.7717\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.2859 - acc: 0.7717\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3883 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3883 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3883 - acc: 0.6304\n",
      "Training Sensitivity :  0.6\n",
      "Training Specificity :  0.835820895522388\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.7142857142857143\n",
      "Current training Accuracy : 0.7717391\n",
      "Current validation Accuracy : 0.6304348\n",
      "Current training loss : 0.2859283685684204\n",
      "Current validation loss : 0.38833050105882727\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.2729 - acc: 0.8098\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.2729 - acc: 0.8098\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.2729 - acc: 0.8098\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3805 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3805 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3805 - acc: 0.6522\n",
      "Training Sensitivity :  0.62\n",
      "Training Specificity :  0.8805970149253731\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.7857142857142857\n",
      "Current training Accuracy : 0.8097826\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.2729129195213318\n",
      "Current validation loss : 0.3805464946705362\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.2739 - acc: 0.8098\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.2739 - acc: 0.8098\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.2739 - acc: 0.8098\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3924 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3924 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3924 - acc: 0.6739\n",
      "Training Sensitivity :  0.6\n",
      "Training Specificity :  0.8880597014925373\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.8097826\n",
      "Current validation Accuracy : 0.67391306\n",
      "Current training loss : 0.2739319853160692\n",
      "Current validation loss : 0.39242582476657367\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.2595 - acc: 0.8152\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.2595 - acc: 0.8152\n",
      "184/184 [==============================] - 0s 114us/sample - loss: 0.2595 - acc: 0.8152\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3837 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3837 - acc: 0.6739\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3837 - acc: 0.6739\n",
      "Training Sensitivity :  0.64\n",
      "Training Specificity :  0.8805970149253731\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.8152174\n",
      "Current validation Accuracy : 0.67391306\n",
      "Current training loss : 0.25946392442869104\n",
      "Current validation loss : 0.38367710942807404\n",
      "184/184 [==============================] - 0s 70us/sample - loss: 0.2566 - acc: 0.8261\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.2566 - acc: 0.8261\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.2566 - acc: 0.8261\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3882 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3882 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3882 - acc: 0.6522\n",
      "Training Sensitivity :  0.66\n",
      "Training Specificity :  0.8880597014925373\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.82608694\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.25660880752231763\n",
      "Current validation loss : 0.3881866646849591\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.2707 - acc: 0.8424\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.2707 - acc: 0.8424\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.2707 - acc: 0.8424\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4253 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4253 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4253 - acc: 0.6522\n",
      "Training Sensitivity :  0.64\n",
      "Training Specificity :  0.917910447761194\n",
      "Validation Sensitivity :  0.2222222222222222\n",
      "Validation Specificity :  0.9285714285714286\n",
      "Current training Accuracy : 0.8423913\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.27074756570484326\n",
      "Current validation loss : 0.4252801356108292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 0s 87us/sample - loss: 0.2517 - acc: 0.8424\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.2517 - acc: 0.8424\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.2517 - acc: 0.8424\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4188 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4188 - acc: 0.6304\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.4188 - acc: 0.6304\n",
      "Training Sensitivity :  0.66\n",
      "Training Specificity :  0.9104477611940298\n",
      "Validation Sensitivity :  0.2222222222222222\n",
      "Validation Specificity :  0.8928571428571429\n",
      "Current training Accuracy : 0.8423913\n",
      "Current validation Accuracy : 0.6304348\n",
      "Current training loss : 0.2517377475033636\n",
      "Current validation loss : 0.41883244721785834\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.2367 - acc: 0.8315\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.2367 - acc: 0.8315\n",
      "184/184 [==============================] - 0s 70us/sample - loss: 0.2367 - acc: 0.8315\n",
      "46/46 [==============================] - 0s 174us/sample - loss: 0.4156 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.4156 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.4156 - acc: 0.6522\n",
      "Training Sensitivity :  0.7\n",
      "Training Specificity :  0.8805970149253731\n",
      "Validation Sensitivity :  0.3333333333333333\n",
      "Validation Specificity :  0.8571428571428571\n",
      "Current training Accuracy : 0.83152175\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.2366822336031043\n",
      "Current validation loss : 0.4156236285748689\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.2204 - acc: 0.8478\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.2204 - acc: 0.8478\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.2204 - acc: 0.8478\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3973 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3973 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3973 - acc: 0.6522\n",
      "Training Sensitivity :  0.72\n",
      "Training Specificity :  0.8955223880597015\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.84782606\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.2203621682913407\n",
      "Current validation loss : 0.3972551226615906\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.2090 - acc: 0.8750\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.2090 - acc: 0.8750\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.2090 - acc: 0.8750\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3807 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 174us/sample - loss: 0.3807 - acc: 0.6522\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3807 - acc: 0.6522\n",
      "Training Sensitivity :  0.76\n",
      "Training Specificity :  0.917910447761194\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.875\n",
      "Current validation Accuracy : 0.65217394\n",
      "Current training loss : 0.20901434317879056\n",
      "Current validation loss : 0.38071205823317816\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.2053 - acc: 0.8696\n",
      "184/184 [==============================] - 0s 98us/sample - loss: 0.2053 - acc: 0.8696\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.2053 - acc: 0.8696\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3581 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3581 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3581 - acc: 0.6957\n",
      "Training Sensitivity :  0.76\n",
      "Training Specificity :  0.9104477611940298\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.8928571428571429\n",
      "Current training Accuracy : 0.8695652\n",
      "Current validation Accuracy : 0.6956522\n",
      "Current training loss : 0.20534699636956918\n",
      "Current validation loss : 0.35805222262506897\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.2010 - acc: 0.8587\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.2010 - acc: 0.8587\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.2010 - acc: 0.8587\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3556 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3556 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.3556 - acc: 0.6957\n",
      "Training Sensitivity :  0.76\n",
      "Training Specificity :  0.8955223880597015\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.8586956\n",
      "Current validation Accuracy : 0.6956522\n",
      "Current training loss : 0.20104115423948868\n",
      "Current validation loss : 0.35556903092757514\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.1962 - acc: 0.8478\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.1962 - acc: 0.8478\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.1962 - acc: 0.8478\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3583 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 174us/sample - loss: 0.3583 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3583 - acc: 0.6957\n",
      "Training Sensitivity :  0.74\n",
      "Training Specificity :  0.8880597014925373\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.84782606\n",
      "Current validation Accuracy : 0.6956522\n",
      "Current training loss : 0.196192917616471\n",
      "Current validation loss : 0.35830657896788226\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.1946 - acc: 0.8424\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.1946 - acc: 0.8424\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.1946 - acc: 0.8424\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3553 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3553 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3553 - acc: 0.6957\n",
      "Training Sensitivity :  0.72\n",
      "Training Specificity :  0.8880597014925373\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.8423913\n",
      "Current validation Accuracy : 0.6956522\n",
      "Current training loss : 0.19462818943935892\n",
      "Current validation loss : 0.35529053988664044\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.1966 - acc: 0.8696\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.1966 - acc: 0.8696\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.1966 - acc: 0.8696\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3564 - acc: 0.7391\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3564 - acc: 0.7391\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.3564 - acc: 0.7391\n",
      "Training Sensitivity :  0.72\n",
      "Training Specificity :  0.9253731343283582\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.8928571428571429\n",
      "Current training Accuracy : 0.8695652\n",
      "Current validation Accuracy : 0.73913044\n",
      "Current training loss : 0.1966257535892984\n",
      "Current validation loss : 0.3563565404518791\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.1958 - acc: 0.8641\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.1958 - acc: 0.8641\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.1958 - acc: 0.8641\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3598 - acc: 0.7391\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3598 - acc: 0.7391\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.3598 - acc: 0.7391\n",
      "Training Sensitivity :  0.68\n",
      "Training Specificity :  0.9328358208955224\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.9285714285714286\n",
      "Current training Accuracy : 0.86413044\n",
      "Current validation Accuracy : 0.73913044\n",
      "Current training loss : 0.19584374583285788\n",
      "Current validation loss : 0.35975772401560907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 0s 81us/sample - loss: 0.1904 - acc: 0.8859\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.1904 - acc: 0.8859\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.1904 - acc: 0.8859\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3516 - acc: 0.7391\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.3516 - acc: 0.7391\n",
      "46/46 [==============================] - 0s 217us/sample - loss: 0.3516 - acc: 0.7391\n",
      "Training Sensitivity :  0.72\n",
      "Training Specificity :  0.9477611940298507\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.9642857142857143\n",
      "Current training Accuracy : 0.88586956\n",
      "Current validation Accuracy : 0.73913044\n",
      "Current training loss : 0.190448926842731\n",
      "Current validation loss : 0.3516426967537921\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.1847 - acc: 0.8913\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.1847 - acc: 0.8913\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.1847 - acc: 0.8913\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3441 - acc: 0.7391\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3441 - acc: 0.7391\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3441 - acc: 0.7391\n",
      "Training Sensitivity :  0.74\n",
      "Training Specificity :  0.9477611940298507\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.9642857142857143\n",
      "Current training Accuracy : 0.8913044\n",
      "Current validation Accuracy : 0.73913044\n",
      "Current training loss : 0.18473678308984506\n",
      "Current validation loss : 0.3441326359043951\n",
      "184/184 [==============================] - 0s 108us/sample - loss: 0.1824 - acc: 0.8967\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.1824 - acc: 0.8967\n",
      "184/184 [==============================] - 0s 70us/sample - loss: 0.1824 - acc: 0.8967\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.3508 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.3508 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3508 - acc: 0.7174\n",
      "Training Sensitivity :  0.72\n",
      "Training Specificity :  0.9626865671641791\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.9285714285714286\n",
      "Current training Accuracy : 0.8967391\n",
      "Current validation Accuracy : 0.7173913\n",
      "Current training loss : 0.18239757029906564\n",
      "Current validation loss : 0.3508448730344358\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.1749 - acc: 0.8913\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.1749 - acc: 0.8913\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.1749 - acc: 0.8913\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3486 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3486 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3486 - acc: 0.6957\n",
      "Training Sensitivity :  0.72\n",
      "Training Specificity :  0.9552238805970149\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.8928571428571429\n",
      "Current training Accuracy : 0.8913044\n",
      "Current validation Accuracy : 0.6956522\n",
      "Current training loss : 0.1748684437378593\n",
      "Current validation loss : 0.348577548628268\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.1667 - acc: 0.8967\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.1667 - acc: 0.8967\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.1667 - acc: 0.8967\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3479 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.3479 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3479 - acc: 0.7174\n",
      "Training Sensitivity :  0.78\n",
      "Training Specificity :  0.9402985074626866\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.8571428571428571\n",
      "Current training Accuracy : 0.8967391\n",
      "Current validation Accuracy : 0.7173913\n",
      "Current training loss : 0.16666891263878864\n",
      "Current validation loss : 0.3479429224263067\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.1619 - acc: 0.8804\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.1619 - acc: 0.8804\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.1619 - acc: 0.8804\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3529 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3529 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3529 - acc: 0.7174\n",
      "Training Sensitivity :  0.76\n",
      "Training Specificity :  0.9253731343283582\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.8571428571428571\n",
      "Current training Accuracy : 0.8804348\n",
      "Current validation Accuracy : 0.7173913\n",
      "Current training loss : 0.1618712129800216\n",
      "Current validation loss : 0.35289295859958814\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.1580 - acc: 0.8804\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.1580 - acc: 0.8804\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.1580 - acc: 0.8804\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3439 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.3439 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3439 - acc: 0.6957\n",
      "Training Sensitivity :  0.76\n",
      "Training Specificity :  0.9253731343283582\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.8804348\n",
      "Current validation Accuracy : 0.6956522\n",
      "Current training loss : 0.15801654950432156\n",
      "Current validation loss : 0.34394529591435974\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.1527 - acc: 0.8859\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.1527 - acc: 0.8859\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.1527 - acc: 0.8859\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3389 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3389 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3389 - acc: 0.6957\n",
      "Training Sensitivity :  0.78\n",
      "Training Specificity :  0.9253731343283582\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.8214285714285714\n",
      "Current training Accuracy : 0.88586956\n",
      "Current validation Accuracy : 0.6956522\n",
      "Current training loss : 0.15269181261891904\n",
      "Current validation loss : 0.33888806467470917\n",
      "184/184 [==============================] - 0s 70us/sample - loss: 0.1617 - acc: 0.8696\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.1617 - acc: 0.8696\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.1617 - acc: 0.8696\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3418 - acc: 0.7391\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3418 - acc: 0.7391\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3418 - acc: 0.7391\n",
      "Training Sensitivity :  0.7\n",
      "Training Specificity :  0.9328358208955224\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.8928571428571429\n",
      "Current training Accuracy : 0.8695652\n",
      "Current validation Accuracy : 0.73913044\n",
      "Current training loss : 0.16169508125471033\n",
      "Current validation loss : 0.34177744129429694\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.1649 - acc: 0.8750\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.1649 - acc: 0.8750\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.1649 - acc: 0.8750\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3545 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3545 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3545 - acc: 0.7174\n",
      "Training Sensitivity :  0.7\n",
      "Training Specificity :  0.9402985074626866\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.8928571428571429\n",
      "Current training Accuracy : 0.875\n",
      "Current validation Accuracy : 0.7173913\n",
      "Current training loss : 0.16491916386977487\n",
      "Current validation loss : 0.35450220626333484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 0s 76us/sample - loss: 0.1652 - acc: 0.8696\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.1652 - acc: 0.8696\n",
      "184/184 [==============================] - 0s 70us/sample - loss: 0.1652 - acc: 0.8696\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3553 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3553 - acc: 0.6957\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3553 - acc: 0.6957\n",
      "Training Sensitivity :  0.7\n",
      "Training Specificity :  0.9328358208955224\n",
      "Validation Sensitivity :  0.3888888888888889\n",
      "Validation Specificity :  0.8928571428571429\n",
      "Current training Accuracy : 0.8695652\n",
      "Current validation Accuracy : 0.6956522\n",
      "Current training loss : 0.16523075622061026\n",
      "Current validation loss : 0.3553111397701761\n",
      "184/184 [==============================] - 0s 76us/sample - loss: 0.1579 - acc: 0.8750\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.1579 - acc: 0.8750\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.1579 - acc: 0.8750\n",
      "46/46 [==============================] - 0s 108us/sample - loss: 0.3595 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3595 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 173us/sample - loss: 0.3595 - acc: 0.7174\n",
      "Training Sensitivity :  0.72\n",
      "Training Specificity :  0.9328358208955224\n",
      "Validation Sensitivity :  0.4444444444444444\n",
      "Validation Specificity :  0.8928571428571429\n",
      "Current training Accuracy : 0.875\n",
      "Current validation Accuracy : 0.7173913\n",
      "Current training loss : 0.15791924103446628\n",
      "Current validation loss : 0.35951114219167957\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.1436 - acc: 0.8859\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.1436 - acc: 0.8859\n",
      "184/184 [==============================] - 0s 87us/sample - loss: 0.1436 - acc: 0.8859\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3521 - acc: 0.7391\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3521 - acc: 0.7391\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3521 - acc: 0.7391\n",
      "Training Sensitivity :  0.8\n",
      "Training Specificity :  0.917910447761194\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.8928571428571429\n",
      "Current training Accuracy : 0.88586956\n",
      "Current validation Accuracy : 0.73913044\n",
      "Current training loss : 0.14361575375432553\n",
      "Current validation loss : 0.352073786051377\n",
      "184/184 [==============================] - 0s 98us/sample - loss: 0.1444 - acc: 0.8859\n",
      "184/184 [==============================] - 0s 81us/sample - loss: 0.1444 - acc: 0.8859\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.1444 - acc: 0.8859\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3430 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 152us/sample - loss: 0.3430 - acc: 0.7174\n",
      "46/46 [==============================] - 0s 130us/sample - loss: 0.3430 - acc: 0.7174\n",
      "Training Sensitivity :  0.82\n",
      "Training Specificity :  0.9104477611940298\n",
      "Validation Sensitivity :  0.5\n",
      "Validation Specificity :  0.8571428571428571\n",
      "Current training Accuracy : 0.88586956\n",
      "Current validation Accuracy : 0.7173913\n",
      "Current training loss : 0.14441107407860135\n",
      "Current validation loss : 0.34298679103022034\n",
      "184/184 [==============================] - 0s 65us/sample - loss: 0.1395 - acc: 0.8859\n",
      "184/184 [==============================] - 0s 92us/sample - loss: 0.1395 - acc: 0.8859\n",
      " 32/184 [====>.........................] - ETA: 0s - loss: 0.1304 - acc: 0.9062"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-2782a4f4a136>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mtrain_acc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mcurr_training_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mtr_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\sipark\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1009\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m           steps=steps)\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m   def predict(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\sipark\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\sipark\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\sipark\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_loss_best=100\n",
    "count_lim=15\n",
    "epoch=0\n",
    "\n",
    "train_accuracy_list=[]\n",
    "val_accuracy_list=[]\n",
    "train_loss_list=[]\n",
    "val_loss_list=[]\n",
    "\n",
    "while 1:\n",
    "    # 모델 학습 시키기\n",
    "    model.fit(train_x, train_y, batch_size=10, epochs=1, verbose = 0)\n",
    "    \n",
    "    # Epoch 증가 시키기\n",
    "    epoch+=1\n",
    "    \n",
    "    # Train Dataset에 대한 Loss 값 저장하기\n",
    "    train_loss=model.evaluate(train_x, train_y)[0]\n",
    "    \n",
    "    # Train Dataset에 대한 Accuracy 값 저장하기\n",
    "    train_acc=model.evaluate(train_x, train_y)[1]\n",
    "    curr_training_accuracy=model.evaluate(train_x, train_y)[1]\n",
    "\n",
    "    # Train Dataset에 대한 모델 예측 값 저장하기\n",
    "    tr_predictions = model.predict(train_x)\n",
    "    \n",
    "    # 모델 예측 값이 0.5보다 크면 1로, 아니면 0으로 레이블하기\n",
    "    labeled_tr_predictions = np.where(tr_predictions > 0.5, 1, 0).flatten()\n",
    "    \n",
    "    # Vadlidation Dataset에 대한 Loss 값 저장하기\n",
    "    val_loss=model.evaluate(val_x, val_y)[0]\n",
    "    \n",
    "    # Validation Dataset에 대한 Accuracy 값 저장하기\n",
    "    val_acc=model.evaluate(val_x, val_y)[1]\n",
    "    curr_validation_accuracy=model.evaluate(val_x, val_y)[1]\n",
    "    \n",
    "    # Validation Dataset에 대한 모델 예측 값 저장하기\n",
    "    val_predictions = model.predict(val_x)\n",
    "    \n",
    "    # 모델 예측 값이 0.5보다 크면 1로, 아니면 0으로 레이블하기\n",
    "    labeled_val_predictions = np.where(val_predictions > 0.5, 1, 0).flatten()\n",
    "    \n",
    "    # Train 및 Validation Dataset에 대한 Sensitivity, Specificity 계산하기\n",
    "    tr_sensitivity, tr_specificity = check_correct(labeled_tr_predictions, train_y.tolist())\n",
    "    val_sensitivity, val_specificity = check_correct(labeled_val_predictions, val_y.tolist())\n",
    "    \n",
    "    # 저장한 값들을 출력하기\n",
    "    print(\"Training Sensitivity : \", tr_sensitivity)\n",
    "    print(\"Training Specificity : \", tr_specificity)\n",
    "    \n",
    "    print(\"Validation Sensitivity : \", val_sensitivity)\n",
    "    print(\"Validation Specificity : \", val_specificity)\n",
    "    \n",
    "    print(\"Current training Accuracy : \"+str(curr_training_accuracy))\n",
    "    print(\"Current validation Accuracy : \"+str(curr_validation_accuracy))\n",
    "    \n",
    "    print(\"Current training loss : \"+str(train_loss))\n",
    "    print(\"Current validation loss : \"+str(val_loss))\n",
    "    \n",
    "    # 현재 Epoch에서의 Train 및 Validation Dataset에 대한 Accuracy와 Loss를 리스트에 추가하기\n",
    "    train_accuracy_list.append(train_acc)\n",
    "    val_accuracy_list.append(val_acc)\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    \n",
    "    # 현재 Validation Loss가 이전의 모든 Validation Loss 보다 낮을 경우,\n",
    "    # val_loss_best 변수를 현재 val_loss로 업데이트 하고,\n",
    "    # count 변수를 초기화\n",
    "    if val_loss < val_loss_best: # new best model. count reset.\n",
    "        val_loss_best = val_loss\n",
    "        count=0\n",
    "        \n",
    "    # Validation Loss가 count_lim 횟수 만큼 개선되지 않았을 경우, 학습 중단하기\n",
    "    if count>count_lim: # no increase, stop.\n",
    "        break\n",
    "    \n",
    "    # count 변수 증가시키기\n",
    "    else: \n",
    "        count=count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8333333333333334, 0.25)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_sensitivity, val_specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train / val loss, Train / val acc 그래프 그려보기\n",
    "fig, loss_aixs = plt.subplots()\n",
    "\n",
    "acc_aixs = loss_aixs.twinx()\n",
    "\n",
    "loss_aixs.plot(train_loss_list, 'y', label='train loss')\n",
    "loss_aixs.plot(val_loss_list, 'r', label='val loss')\n",
    "\n",
    "acc_aixs.plot(train_accuracy_list, 'b', label='train acc')\n",
    "acc_aixs.plot(val_accuracy_list, 'g', label='val acc')\n",
    "\n",
    "loss_aixs.set_xlabel('epoch')\n",
    "loss_aixs.set_ylabel('loss')\n",
    "acc_aixs.set_ylabel('accuracy')\n",
    "\n",
    "loss_aixs.legend(loc='upper left')\n",
    "acc_aixs.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184/184 [==============================] - 0s 92us/sample - loss: 0.0722 - acc: 0.9511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07223690074423085, 0.95108694]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Dataset에 대한 모델의 Loss와 Accuracy 값 리턴\n",
    "model.evaluate(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 152us/sample - loss: 0.2913 - acc: 0.7609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2913026213645935, 0.76086956]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Dataset에 대한 모델의 Loss와 Accuracy 값 리턴\n",
    "model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataset에 대해 모델 예측\n",
    "train_h = model.predict(train_x)\n",
    "train_p = (train_h > 0.5).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset에 대해 모델 예측\n",
    "test_h = model.predict(test_x)\n",
    "test_p = (test_h > 0.5).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test Dataset에 대해 Sensitivity, Specificity 계산하고 출력해보기\n",
    "tr_sensitivity, tr_specificity = check_correct(train_p, train_y)\n",
    "ts_sensitivity, ts_specificity = check_correct(test_p, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.94, 0.9552238805970149)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_sensitivity, tr_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5555555555555556, 0.8928571428571429)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_sensitivity, ts_specificity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
